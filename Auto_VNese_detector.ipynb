{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2ab2f-6954-49da-a8ff-c8051f253eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF, transformers==4.50.3, torch==2.6.0, pytesseract, pdf2image, opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ea044-3728-427e-a18f-607d01effb1f",
   "metadata": {},
   "source": [
    "# Final V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6decdf-7aca-4924-8685-86a9be564c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.65):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "        # NEW: Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()  # Changed variable name\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords           \n",
    "        \"bị\", \"bởi\", \"cả\", \"các\", \"có\", \"có_thể\", \"có_lẽ\", \"của\",\n",
    "        \"cùng\",\"cùng_với\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đều\", \"do\", \"đó\",\n",
    "        \"khi\", \"là\", \"lại\", \"mà\", \"nên\", \"nếu\", \"những\",\n",
    "        \"phải\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"thì\", \"từ\", \"và\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "    \n",
    "    def segment_words(self, text):\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        # Use segmentRawSentences to process the text\n",
    "        segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "        return segmented_text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Vietnamese characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        # Clean text (unchanged)\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords (unchanged)\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Use vws for word segmentation instead of simple split\n",
    "        segmented_text = self.segment_words(text)\n",
    "        \n",
    "        # Return segment words\n",
    "        return segmented_text\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract and preprocess text from PDF, handling both vector and image-only pages\"\"\"\n",
    "\n",
    "        full_text = []\n",
    "        try:\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                for page in doc:\n",
    "                    # === 1. Extract Vector Text ===\n",
    "                    text = \"\"\n",
    "                    blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                    for block in blocks:\n",
    "                        if \"lines\" in block:\n",
    "                            for line in block[\"lines\"]:\n",
    "                                for span in line[\"spans\"]:\n",
    "                                    text += span[\"text\"] + \" \"\n",
    "                    full_text.append(text.strip())\n",
    "        \n",
    "                    # === 2. OCR Page as Image (Visual Handling) ===\n",
    "                    try:\n",
    "                        zoom = 2.0  # Higher DPI (144 DPI)\n",
    "                        mat = fitz.Matrix(zoom, zoom)\n",
    "                        pix = page.get_pixmap(matrix=mat)\n",
    "        \n",
    "                        img_np = np.frombuffer(pix.samples, dtype=np.uint8).reshape((pix.height, pix.width, pix.n))\n",
    "                        if pix.n == 4:\n",
    "                            img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2BGR)\n",
    "                        elif pix.n == 1:\n",
    "                            img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "                        # Quick precheck (low-cost confidence screening)\n",
    "                        quick_ocr = pytesseract.image_to_data(\n",
    "                            img_np, lang='vie', config='--oem 3 --psm 6', output_type=pytesseract.Output.DICT\n",
    "                        )\n",
    "                        quick_valid_chars = sum(len(word.strip()) for word, conf in zip(quick_ocr['text'], quick_ocr['conf']) if word.strip() and int(conf) > 60)\n",
    "        \n",
    "                        if quick_valid_chars < 6:\n",
    "                            continue\n",
    "        \n",
    "                        # === Preprocessing for final OCR ===\n",
    "                        gray = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
    "                        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "                        gray = clahe.apply(gray)\n",
    "        \n",
    "                        # Upscale if too small\n",
    "                        if gray.shape[0] < 1000:\n",
    "                            scale = 300 / 72\n",
    "                            gray = cv2.resize(gray, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "                        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "        \n",
    "                        coords = np.column_stack(np.where(thresh > 0))\n",
    "                        angle = cv2.minAreaRect(coords)[-1]\n",
    "                        if angle < -45:\n",
    "                            angle += 90\n",
    "                        elif angle > 45:\n",
    "                            angle -= 90\n",
    "                        (h, w) = thresh.shape\n",
    "                        M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "                        deskewed = cv2.warpAffine(thresh, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "        \n",
    "                        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "                        processed = cv2.morphologyEx(deskewed, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "                        d = pytesseract.image_to_data(\n",
    "                            processed, lang='vie', config='--oem 3 --psm 6', output_type=pytesseract.Output.DICT\n",
    "                        )\n",
    "        \n",
    "                        # === Filter out meaningless pages ===\n",
    "                        valid_word_count = 0\n",
    "                        valid_char_count = 0\n",
    "                        for i in range(len(d['text'])):\n",
    "                            word = d['text'][i].strip()\n",
    "                            conf = int(d['conf'][i])\n",
    "                            if conf >= 70 and len(word) >= 3:\n",
    "                                valid_word_count += 1\n",
    "                                valid_char_count += len(word)\n",
    "        \n",
    "                        if valid_word_count < 3 or valid_char_count < 17:\n",
    "                            continue\n",
    "        \n",
    "                        # === Structure OCR Text ===\n",
    "                        blocks = defaultdict(lambda: defaultdict(list))\n",
    "                        for i in range(len(d['text'])):\n",
    "                            if int(d['conf'][i]) > 0 and d['text'][i].strip():\n",
    "                                block_id = d['block_num'][i]\n",
    "                                line_id = d['line_num'][i]\n",
    "                                word_info = {\n",
    "                                    'text': d['text'][i],\n",
    "                                    'left': d['left'][i],\n",
    "                                    'top': d['top'][i]\n",
    "                                }\n",
    "                                blocks[block_id][line_id].append(word_info)\n",
    "        \n",
    "                        sorted_blocks = sorted(blocks.items(), key=lambda b: min([w['top'] for line in b[1].values() for w in line]))\n",
    "                        structured_text = \"\"\n",
    "                        for block_id, lines in sorted_blocks:\n",
    "                            sorted_lines = sorted(lines.items(), key=lambda l: min(w['top'] for w in l[1]))\n",
    "                            for line_id, words in sorted_lines:\n",
    "                                sorted_words = sorted(words, key=lambda w: w['left'])\n",
    "                                line_text = \" \".join([w['text'] for w in sorted_words])\n",
    "                                structured_text += line_text.strip() + \"\\n\"\n",
    "        \n",
    "                        full_text.append(structured_text)\n",
    "        \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Error rendering or OCRing page {page.number}: {e}\")\n",
    "                        continue\n",
    "        \n",
    "            return \"\\n\".join(full_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped empty file: {pdf_path}\")\n",
    "            \n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=1):\n",
    "        \"\"\"Get embeddings in batches\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            \n",
    "            encoded = self.pho_tokenizer(batch, \n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            \n",
    "                # Use mean pooling instead of just [CLS] token\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            batch_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            \n",
    "\n",
    "            # L2 normalization for better cosine similarity\n",
    "            batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "            batch_embeddings = batch_embeddings.numpy()\n",
    "            \n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def read_criteria(self, excel_path):\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()           # A list of criteria is collected\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]      # Process 1 item per time in a list of criteria\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "\n",
    "    def chunk_into_sentences(self, text):\n",
    "        \"\"\"Chunk the text into sentences based on delimiters (. ? ! ; ...).\"\"\"\n",
    "        sentence_endings = re.compile(r'([.!?;]{1,3})')  # Regex to match sentence-ending delimiters\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        sentences = [sentences[i].strip() + (sentences[i + 1] if i + 1 < len(sentences) else '')\n",
    "                     for i in range(0, len(sentences), 2)]  # Only take sentences (even index)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "    def analyze_single_pdf(self, pdf_path, criteria_phrases):\n",
    "        \"\"\"Analyze a single PDF file against criteria phrases\"\"\"\n",
    "        # Extract raw pdf text\n",
    "        pdf_text = self.extract_text_from_pdf(pdf_path)      # full pdf file, raw and original\n",
    "        if not pdf_text:\n",
    "            return {phrase: \"x\" for phrase in criteria_phrases}\n",
    "\n",
    "                 \n",
    "        # Initialize results dictionary\n",
    "        results = {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # First, perform exact matching\n",
    "        pdf_text_lower = pdf_text.lower()\n",
    "        for phrase in criteria_phrases:\n",
    "            clean_phrase = self.clean_text(phrase).lower()\n",
    "            if clean_phrase in pdf_text_lower:\n",
    "                results[phrase] = 1\n",
    "        \n",
    "        # Get list of unmatched phrases for semantic analysis\n",
    "        unmatched_phrases = [phrase for phrase in criteria_phrases if results[phrase] == 0]\n",
    "        \n",
    "        # If all phrases were matched exactly, return results\n",
    "        if not unmatched_phrases:\n",
    "            return results\n",
    "            \n",
    "        # Otherwise, proceed with semantic similarity analysis for unmatched phrases\n",
    "        sentences = self.chunk_into_sentences(pdf_text)\n",
    "        if not sentences:\n",
    "            return results\n",
    "    \n",
    "        # Preprocess_text so it is cleaned\n",
    "        processed_text = [self.preprocess_text(senten) for senten in sentences]\n",
    "        \n",
    "        # Get embeddings only for unmatched phrases\n",
    "        try:\n",
    "            unmatched_embeddings = [self.get_embeddings(phrase) for phrase in unmatched_phrases]\n",
    "            \n",
    "            # Check semantic similarity only for unmatched phrases\n",
    "            for sentence in processed_text:\n",
    "                sentence_embedding = self.get_embeddings(sentence)\n",
    "                if len(sentence_embedding.shape) == 1:\n",
    "                    sentence_embedding = sentence_embedding.reshape(1, -1)\n",
    "                for i, criterion_embedding in enumerate(unmatched_embeddings):\n",
    "                    similarity = cosine_similarity(sentence_embedding, criterion_embedding)[0][0]\n",
    "                    if similarity >= self.similarity_threshold:\n",
    "                        results[unmatched_phrases[i]] = 1\n",
    "                \n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic analysis: {e}\")\n",
    "            return results\n",
    "\n",
    "    \n",
    "    def analyze_multiple_pdfs(self, pdf_folder, excel_path, output_path):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        # Read criteria\n",
    "        criteria_phrases = self.read_criteria(excel_path)\n",
    "        if not criteria_phrases:\n",
    "            print(\"No criteria found. Exiting...\")\n",
    "            return\n",
    "\n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the specified folder.\")\n",
    "            return\n",
    "            \n",
    "        all_results = []   \n",
    "        for idx, pdf_file in enumerate(pdf_files, 1):  # Start index at 1\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        \n",
    "            # Analyze PDF\n",
    "            results = self.analyze_single_pdf(pdf_path, criteria_phrases)\n",
    "            results['PDF File'] = pdf_file\n",
    "            all_results.append(results)\n",
    "        \n",
    "            # Save after every 2 files OR at the end of the list\n",
    "            if idx % 2 == 0 or idx == len(pdf_files): \n",
    "                try:\n",
    "                    results_df = pd.DataFrame(all_results)\n",
    "                    results_df.to_excel(output_path, index=False)\n",
    "                    print(f\"Intermediate results saved to {output_path} after processing {idx} PDFs.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving results after {idx} PDFs: {e}\")\n",
    "            \n",
    "#Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.65)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/esg_words.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_pdf_reading.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
