{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7dad5d-cfbb-4028-8cff-753be8b88d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (4.50.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c011be1d-c071-4c87-8401-62c865a83c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/16.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/16.6 MB 2.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.3/16.6 MB 2.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 2.1/16.6 MB 2.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.9/16.6 MB 2.9 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.7/16.6 MB 3.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.7/16.6 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 5.8/16.6 MB 3.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 6.6/16.6 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.6/16.6 MB 3.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.4/16.6 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 9.2/16.6 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 10.0/16.6 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 11.0/16.6 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.8/16.6 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.6/16.6 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.4/16.6 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.9/16.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.2/16.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.25.5\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e9feed-5170-4928-b70d-9ae7997492ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fbd554-a871-4c5c-bb62-792e78dd6d32",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (117876230.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Dưới đây là cách triển khai mô hình nhận diện từ khóa và ngữ cảnh trong các file PDF tiếng Việt bằng Python. Chúng ta sẽ sử dụng các thư viện phổ biến như `pandas`, `PyMuPDF`, `pdfminer.six`, `VnCoreNLP` và `transformers` (PhoBERT) để phân tích và xử lý văn bản.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Cài đặt thư viện cần thiết**\n",
    "Trước tiên, hãy cài đặt các thư viện cần dùng:\n",
    "```bash\n",
    "pip install pandas pymupdf pdfminer.six transformers vncorenlp underthesea torch\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Đọc danh sách từ khóa từ file Excel**\n",
    "Chúng ta sử dụng `pandas` để đọc danh sách từ khóa từ file Excel:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Đọc file Excel chứa danh sách từ khóa\n",
    "file_excel = \"keywords.xlsx\"\n",
    "df_keywords = pd.read_excel(file_excel)\n",
    "keywords = df_keywords['Keyword'].tolist()  # Lấy danh sách từ khóa\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Trích xuất văn bản từ file PDF**\n",
    "Ta có thể dùng `PyMuPDF` hoặc `pdfminer.six` để lấy văn bản từ file PDF:\n",
    "```python\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Đọc nội dung từ file PDF\n",
    "pdf_file = \"company_report.pdf\"\n",
    "document_text = extract_text_from_pdf(pdf_file)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Tiền xử lý văn bản**\n",
    "Ta làm sạch văn bản để chuẩn bị cho phân tích:\n",
    "```python\n",
    "import re\n",
    "import underthesea  # Dùng để tách từ tiếng Việt\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Chuyển về chữ thường\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Xóa khoảng trắng dư thừa\n",
    "    words = underthesea.word_tokenize(text)  # Tách từ tiếng Việt\n",
    "    return words\n",
    "\n",
    "processed_text = preprocess_text(document_text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Tìm kiếm từ khóa và gán nhãn**\n",
    "Chúng ta kiểm tra xem từ khóa có xuất hiện trong văn bản hay không. Nếu có, gán nhãn `1`, nếu không gán `0`:\n",
    "```python\n",
    "def check_keywords(text, keywords):\n",
    "    found_keywords = []\n",
    "    for keyword in keywords:\n",
    "        if keyword in text:\n",
    "            found_keywords.append(keyword)\n",
    "    return 1 if found_keywords else 0  # Gán 1 nếu tìm thấy từ khóa\n",
    "\n",
    "label = check_keywords(processed_text, keywords)\n",
    "print(f\"Kết quả phân tích: {label}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Mở rộng tìm kiếm với PhoBERT để nhận diện ngữ cảnh**\n",
    "Nếu muốn mở rộng khả năng nhận diện ngữ cảnh của từ khóa, ta sử dụng **PhoBERT**, một mô hình NLP mạnh mẽ dành cho tiếng Việt:\n",
    "```python\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Khởi tạo PhoBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "def analyze_context(text, keywords):\n",
    "    tokens = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    output = model(**tokens)\n",
    "\n",
    "# Kiểm tra sự xuất hiện của từ khóa trong các embeddings (giản lược)\n",
    "    for keyword in keywords:\n",
    "        if keyword in text:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "label_context = analyze_context(document_text, keywords)\n",
    "print(f\"Kết quả phân tích ngữ cảnh: {label_context}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Xuất kết quả ra file**\n",
    "Cuối cùng, ta có thể xuất kết quả ra file CSV để dễ dàng theo dõi:\n",
    "```python\n",
    "result_df = pd.DataFrame({'File': [pdf_file], 'Label': [label], 'Context_Label': [label_context]})\n",
    "result_df.to_csv(\"output.csv\", index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Tóm tắt**\n",
    "- **Đọc danh sách từ khóa** từ Excel.\n",
    "- **Trích xuất văn bản** từ PDF.\n",
    "- **Tiền xử lý** để chuẩn hóa dữ liệu.\n",
    "- **Tìm kiếm từ khóa trực tiếp** trong văn bản.\n",
    "- **Mở rộng tìm kiếm** bằng PhoBERT để nhận diện ngữ cảnh.\n",
    "- **Xuất kết quả** ra file CSV.\n",
    "\n",
    "Nếu bạn muốn cải thiện mô hình, có thể áp dụng kỹ thuật **embedding nâng cao** hoặc **fine-tuning PhoBERT** để tăng độ chính xác."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36275695-fc36-4a3b-9195-2c4a6ad6450e",
   "metadata": {},
   "source": [
    "- Chỉnh cosine giữa 0.6 và 0.7\n",
    "- Chỉnh cách ngắt câu + cách parse file pdf\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16cfee08-6116-45e8-bbb2-bf3bcb93f504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TH_milk.pdf...\n",
      "Processing VN_milk.pdf...\n",
      "Results saved to esg_analysis_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.65):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "        # NEW: Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()  # Changed variable name\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords\n",
    "            \"bị\", \"bởi\", \"cả\", \"các\", \"cái\", \"cần\", \"càng\", \"chỉ\", \"chiếc\", \"cho\", \"chứ\", \"chưa\", \"chuyện\", \n",
    "            \"có\", \"có_thể\", \"cứ\", \"của\", \"cùng\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đến_nỗi\", \"đều\", \"điều\", \n",
    "            \"do\", \"đó\", \"được\", \"dưới\", \"gì\", \"khi\", \"không\", \"là\", \"lại\", \"lên\", \"lúc\", \"mà\", \"mỗi\", \n",
    "            \"một_cách\", \"này\", \"nên\", \"nếu\", \"ngay\", \"nhiều\", \"như\", \"nhưng\", \"những\", \"nơi\", \"nữa\", \"phải\", \n",
    "            \"qua\", \"ra\", \"rằng\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"so\", \"sự\", \"tại\", \"theo\", \"thì\", \"trên\", \n",
    "            \"trước\", \"từ\", \"từng\", \"và\", \"vẫn\", \"vào\", \"vậy\", \"vì\", \"việc\", \"với\", \"vừa\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "    \n",
    "    def segment_words(self, text):\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        # Use segmentRawSentences to process the text\n",
    "        segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "        return segmented_text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Vietnamese characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        # Clean text (unchanged)\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords (unchanged)\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Use vws for word segmentation instead of simple split\n",
    "        segmented_text = self.segment_words(text)\n",
    "        \n",
    "        # Return segment words\n",
    "        return segmented_text\n",
    "\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract and preprocess text from PDF file\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            full_text = []\n",
    "            \n",
    "            for page in doc:\n",
    "                text = page.get_text(\"text\")\n",
    "                processed_text = self.preprocess_text(text)\n",
    "                full_text.append(processed_text)\n",
    "            \n",
    "            doc.close()\n",
    "            return ' '.join(full_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF {pdf_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=32):\n",
    "        \"\"\"Get embeddings in batches\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            processed_batch = [self.preprocess_text(text) for text in batch]\n",
    "            \n",
    "            encoded = self.pho_tokenizer(processed_batch, \n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            \n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def read_criteria(self, excel_path):\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def analyze_single_pdf(self, pdf_path, criteria_phrases):\n",
    "        \"\"\"Analyze a single PDF file against criteria phrases\"\"\"\n",
    "        # Extract and preprocess text\n",
    "        pdf_text = self.extract_text_from_pdf(pdf_path)\n",
    "        if not pdf_text:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunk_size = 1000\n",
    "        chunks = [pdf_text[i:i+chunk_size] for i in range(0, len(pdf_text), chunk_size)]\n",
    "        if not chunks:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Get embeddings\n",
    "        try:\n",
    "            criteria_embeddings = self.get_embeddings(criteria_phrases)\n",
    "            chunk_embeddings = self.get_embeddings(chunks)\n",
    "            \n",
    "            # Ensure proper shapes\n",
    "            if len(criteria_embeddings.shape) == 1:\n",
    "                criteria_embeddings = criteria_embeddings.reshape(1, -1)\n",
    "            if len(chunk_embeddings.shape) == 1:\n",
    "                chunk_embeddings = chunk_embeddings.reshape(1, -1)\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = cosine_similarity(criteria_embeddings, chunk_embeddings)\n",
    "            \n",
    "            # Initialize and update results\n",
    "            results = {}\n",
    "            for i, phrase in enumerate(criteria_phrases):\n",
    "                results[phrase] = 1 if np.max(similarities[i]) >= self.similarity_threshold else 0\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in analysis: {e}\")\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "    \n",
    "    def analyze_multiple_pdfs(self, pdf_folder, excel_path, output_path):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        # Read criteria\n",
    "        criteria_phrases = self.read_criteria(excel_path)\n",
    "        if not criteria_phrases:\n",
    "            print(\"No criteria found. Exiting...\")\n",
    "            return\n",
    "        \n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the specified folder.\")\n",
    "            return\n",
    "        \n",
    "        # Process each PDF\n",
    "        all_results = []\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            \n",
    "            # Analyze PDF\n",
    "            results = self.analyze_single_pdf(pdf_path, criteria_phrases)\n",
    "            results['PDF File'] = pdf_file\n",
    "            all_results.append(results)\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            results_df.to_excel(output_path, index=False)\n",
    "            print(f\"Results saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.65)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/words.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_analysis_results.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "    analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513781f8-b86e-456f-afc7-7b74c6cf036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (0.11.6)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from pdfplumber) (20250327)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (43.0.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from pytesseract) (24.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.21)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e8982f-8409-4fc9-a983-7d75dcd27772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TH_milk.pdf...\n",
      "Processing VN_milk.pdf...\n",
      "Results saved to esg_analysis_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.65):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "        # NEW: Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()  # Changed variable name\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords\n",
    "            \"bị\", \"bởi\", \"cả\", \"các\", \"cái\", \"cần\", \"càng\", \"chỉ\", \"chiếc\", \"cho\", \"chứ\", \"chưa\", \"chuyện\", \n",
    "            \"có\", \"có_thể\", \"cứ\", \"của\", \"cùng\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đến_nỗi\", \"đều\", \"điều\", \n",
    "            \"do\", \"đó\", \"được\", \"dưới\", \"gì\", \"khi\", \"không\", \"là\", \"lại\", \"lên\", \"lúc\", \"mà\", \"mỗi\", \n",
    "            \"một_cách\", \"này\", \"nên\", \"nếu\", \"ngay\", \"nhiều\", \"như\", \"nhưng\", \"những\", \"nơi\", \"nữa\", \"phải\", \n",
    "            \"qua\", \"ra\", \"rằng\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"so\", \"sự\", \"tại\", \"theo\", \"thì\", \"trên\", \n",
    "            \"trước\", \"từ\", \"từng\", \"và\", \"vẫn\", \"vào\", \"vậy\", \"vì\", \"việc\", \"với\", \"vừa\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "    \n",
    "    def segment_words(self, text):\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        # Use segmentRawSentences to process the text\n",
    "        segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "        return segmented_text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Vietnamese characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        # Clean text (unchanged)\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords (unchanged)\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Use vws for word segmentation instead of simple split\n",
    "        segmented_text = self.segment_words(text)\n",
    "        \n",
    "        # Return segment words\n",
    "        return segmented_text\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Enhanced PDF text extraction\"\"\"\n",
    "        try:\n",
    "            # Method 1: Regular text extraction\n",
    "            doc = fitz.open(pdf_path)\n",
    "            regular_text = []\n",
    "            for page in doc:\n",
    "                text = page.get_text(\"text\")\n",
    "                regular_text.append(text)\n",
    "            doc.close()\n",
    "            \n",
    "            # Method 2: Layout-aware extraction\n",
    "            import pdfplumber\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                layout_text = []\n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text(layout=True)\n",
    "                    layout_text.append(text)\n",
    "            \n",
    "            # Combine both methods\n",
    "            combined_text = []\n",
    "            for reg, lay in zip(regular_text, layout_text):\n",
    "                # Use layout text if it's longer (might contain more information)\n",
    "                combined_text.append(lay if len(lay) > len(reg) else reg)\n",
    "            \n",
    "            # Process the combined text\n",
    "            processed_text = []\n",
    "            for text in combined_text:\n",
    "                processed = self.preprocess_text(text)\n",
    "                processed_text.append(processed)\n",
    "            \n",
    "            return ' '.join(processed_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF {pdf_path}: {e}\")\n",
    "            return \"\"\n",
    "   \n",
    "    def get_embeddings(self, texts, batch_size=32):\n",
    "        \"\"\"Get embeddings in batches\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            processed_batch = [self.preprocess_text(text) for text in batch]\n",
    "            \n",
    "            encoded = self.pho_tokenizer(processed_batch, \n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            \n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def read_criteria(self, excel_path):\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def analyze_single_pdf(self, pdf_path, criteria_phrases):\n",
    "        \"\"\"Analyze a single PDF file against criteria phrases\"\"\"\n",
    "        # Extract and preprocess text\n",
    "        pdf_text = self.extract_text_from_pdf(pdf_path)\n",
    "        if not pdf_text:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunk_size = 1000\n",
    "        chunks = [pdf_text[i:i+chunk_size] for i in range(0, len(pdf_text), chunk_size)]\n",
    "        if not chunks:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Get embeddings\n",
    "        try:\n",
    "            criteria_embeddings = self.get_embeddings(criteria_phrases)\n",
    "            chunk_embeddings = self.get_embeddings(chunks)\n",
    "            \n",
    "            # Ensure proper shapes\n",
    "            if len(criteria_embeddings.shape) == 1:\n",
    "                criteria_embeddings = criteria_embeddings.reshape(1, -1)\n",
    "            if len(chunk_embeddings.shape) == 1:\n",
    "                chunk_embeddings = chunk_embeddings.reshape(1, -1)\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = cosine_similarity(criteria_embeddings, chunk_embeddings)\n",
    "            \n",
    "            # Initialize and update results\n",
    "            results = {}\n",
    "            for i, phrase in enumerate(criteria_phrases):\n",
    "                results[phrase] = 1 if np.max(similarities[i]) >= self.similarity_threshold else 0\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in analysis: {e}\")\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "    \n",
    "    def analyze_multiple_pdfs(self, pdf_folder, excel_path, output_path):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        # Read criteria\n",
    "        criteria_phrases = self.read_criteria(excel_path)\n",
    "        if not criteria_phrases:\n",
    "            print(\"No criteria found. Exiting...\")\n",
    "            return\n",
    "        \n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the specified folder.\")\n",
    "            return\n",
    "        \n",
    "        # Process each PDF\n",
    "        all_results = []\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            \n",
    "            # Analyze PDF\n",
    "            results = self.analyze_single_pdf(pdf_path, criteria_phrases)\n",
    "            results['PDF File'] = pdf_file\n",
    "            all_results.append(results)\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            results_df.to_excel(output_path, index=False)\n",
    "            print(f\"Results saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.65)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/words.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_analysis_results.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "    analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee83422-c45e-4bad-8570-b4ad9374c6f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing PDFs:   0%|                                                                           | 0/2 [00:00<?, ?it/s]INFO:__main__:Processing TH_milk.pdf...\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "ERROR:__main__:Error in chunk creation: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\VNC/nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\Downloads\\\\Anaconda jpt\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\VNC\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Processing PDFs:   0%|                                                                           | 0/2 [01:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from typing import List, Dict, Tuple\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.65):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "        \n",
    "        # Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords\n",
    "            \"bị\", \"bởi\", \"cả\", \"các\", \"cái\", \"cần\", \"càng\", \"chỉ\", \"chiếc\", \"cho\", \"chứ\", \"chưa\", \"chuyện\", \n",
    "            \"có\", \"có_thể\", \"cứ\", \"của\", \"cùng\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đến_nỗi\", \"đều\", \"điều\", \n",
    "            \"do\", \"đó\", \"được\", \"dưới\", \"gì\", \"khi\", \"không\", \"là\", \"lại\", \"lên\", \"lúc\", \"mà\", \"mỗi\", \n",
    "            \"một_cách\", \"này\", \"nên\", \"nếu\", \"ngay\", \"nhiều\", \"như\", \"nhưng\", \"những\", \"nơi\", \"nữa\", \"phải\", \n",
    "            \"qua\", \"ra\", \"rằng\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"so\", \"sự\", \"tại\", \"theo\", \"thì\", \"trên\", \n",
    "            \"trước\", \"từ\", \"từng\", \"và\", \"vẫn\", \"vào\", \"vậy\", \"vì\", \"việc\", \"với\", \"vừa\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.chunk_overlap = 200  # Number of characters to overlap between chunks\n",
    "        self.max_chunk_size = 1000  # Maximum size of each chunk\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def segment_words(self, text: str) -> str:\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        try:\n",
    "            segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "            return segmented_text\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in word segmentation: {e}\")\n",
    "            return text\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        try:\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "            \n",
    "            # Remove special characters but keep Vietnamese characters and numbers\n",
    "            text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "            \n",
    "            # Remove extra whitespace\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = text.strip()\n",
    "            \n",
    "            return text\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in text cleaning: {e}\")\n",
    "            return text\n",
    "    \n",
    "    def remove_stopwords(self, text: str) -> str:\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        try:\n",
    "            words = text.split()\n",
    "            filtered_words = [word for word in words if word not in self.stopwords]\n",
    "            return ' '.join(filtered_words)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in stopword removal: {e}\")\n",
    "            return text\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        try:\n",
    "            # Clean text\n",
    "            text = self.clean_text(text)\n",
    "            \n",
    "            # Remove stopwords\n",
    "            text = self.remove_stopwords(text)\n",
    "            \n",
    "            # Use vws for word segmentation\n",
    "            segmented_text = self.segment_words(text)\n",
    "            \n",
    "            return segmented_text\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in text preprocessing: {e}\")\n",
    "            return text\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> List[str]:\n",
    "        \"\"\"Enhanced PDF text extraction with structure preservation\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            pages_content = []\n",
    "            \n",
    "            for page_num, page in enumerate(doc):\n",
    "                # Extract text with layout preservation\n",
    "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                page_text = []\n",
    "                \n",
    "                for block in blocks:\n",
    "                    if \"lines\" in block:\n",
    "                        for line in block[\"lines\"]:\n",
    "                            if \"spans\" in line:\n",
    "                                line_text = \" \".join(span[\"text\"] for span in line[\"spans\"])\n",
    "                                if line_text.strip():\n",
    "                                    page_text.append(line_text)\n",
    "                \n",
    "                # Join lines with proper spacing\n",
    "                page_content = \" \".join(page_text)\n",
    "                if page_content.strip():\n",
    "                    pages_content.append(page_content)\n",
    "            \n",
    "            doc.close()\n",
    "            return pages_content\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing PDF {pdf_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_smart_chunks(self, text: str) -> List[str]:\n",
    "        \"\"\"Create chunks based on natural boundaries\"\"\"\n",
    "        try:\n",
    "            chunks = []\n",
    "            sentences = sent_tokenize(text)\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                sentence_size = len(sentence)\n",
    "                \n",
    "                if current_size + sentence_size > self.max_chunk_size and current_chunk:\n",
    "                    # Join current chunk and add to chunks\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    # Keep last few sentences for overlap\n",
    "                    overlap_sentences = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk\n",
    "                    current_chunk = overlap_sentences\n",
    "                    current_size = sum(len(s) for s in overlap_sentences)\n",
    "                \n",
    "                current_chunk.append(sentence)\n",
    "                current_size += sentence_size\n",
    "            \n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            \n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in chunk creation: {e}\")\n",
    "            return [text]\n",
    "\n",
    "    def get_embeddings(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Enhanced embedding generation with normalization\"\"\"\n",
    "        try:\n",
    "            if isinstance(texts, str):\n",
    "                texts = [texts]\n",
    "            \n",
    "            all_embeddings = []\n",
    "            \n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i + batch_size]\n",
    "                processed_batch = [self.preprocess_text(text) for text in batch]\n",
    "                \n",
    "                encoded = self.pho_tokenizer(processed_batch, \n",
    "                                           padding=True, \n",
    "                                           truncation=True,\n",
    "                                           return_tensors=\"pt\",\n",
    "                                           max_length=256)\n",
    "                \n",
    "                # Move inputs to device\n",
    "                encoded = {k: v.to(self.device) for k, v in encoded.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**encoded)\n",
    "                \n",
    "                # Get CLS token embeddings and normalize them\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                # L2 normalization\n",
    "                batch_embeddings = batch_embeddings / np.linalg.norm(batch_embeddings, axis=1, keepdims=True)\n",
    "                all_embeddings.append(batch_embeddings)\n",
    "            \n",
    "            return np.vstack(all_embeddings)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in embedding generation: {e}\")\n",
    "            return np.array([])\n",
    "\n",
    "    def calculate_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate similarity using multiple metrics\"\"\"\n",
    "        try:\n",
    "            # Cosine similarity\n",
    "            cos_sim = cosine_similarity(vec1.reshape(1, -1), vec2.reshape(1, -1))[0][0]\n",
    "            \n",
    "            # L2 distance (normalized)\n",
    "            l2_dist = np.linalg.norm(vec1 - vec2)\n",
    "            l2_sim = 1 / (1 + l2_dist)\n",
    "            \n",
    "            # Combine metrics (weighted average)\n",
    "            combined_sim = 0.7 * cos_sim + 0.3 * l2_sim\n",
    "            return combined_sim\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in similarity calculation: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def analyze_single_pdf(self, pdf_path: str, criteria_phrases: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"Enhanced PDF analysis with improved chunking and similarity calculation\"\"\"\n",
    "        try:\n",
    "            # Extract text with structure preservation\n",
    "            pages_content = self.extract_text_from_pdf(pdf_path)\n",
    "            if not pages_content:\n",
    "                return {phrase: 0 for phrase in criteria_phrases}\n",
    "            \n",
    "            # Create smart chunks for each page\n",
    "            all_chunks = []\n",
    "            for page_content in pages_content:\n",
    "                chunks = self.create_smart_chunks(page_content)\n",
    "                all_chunks.extend(chunks)\n",
    "            \n",
    "            if not all_chunks:\n",
    "                return {phrase: 0 for phrase in criteria_phrases}\n",
    "            \n",
    "            # Get embeddings\n",
    "            criteria_embeddings = self.get_embeddings(criteria_phrases)\n",
    "            chunk_embeddings = self.get_embeddings(all_chunks)\n",
    "            \n",
    "            if criteria_embeddings.size == 0 or chunk_embeddings.size == 0:\n",
    "                return {phrase: 0 for phrase in criteria_phrases}\n",
    "            \n",
    "            # Calculate similarities for each criteria phrase\n",
    "            results = {}\n",
    "            for i, phrase in enumerate(criteria_phrases):\n",
    "                phrase_embedding = criteria_embeddings[i]\n",
    "                max_similarity = 0\n",
    "                \n",
    "                for chunk_embedding in chunk_embeddings:\n",
    "                    similarity = self.calculate_similarity(phrase_embedding, chunk_embedding)\n",
    "                    max_similarity = max(max_similarity, similarity)\n",
    "                \n",
    "                results[phrase] = 1 if max_similarity >= self.similarity_threshold else 0\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in PDF analysis: {e}\")\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "\n",
    "    def read_criteria(self, excel_path: str) -> List[str]:\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "\n",
    "    def analyze_multiple_pdfs(self, pdf_folder: str, excel_path: str, output_path: str):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        try:\n",
    "            # Read criteria\n",
    "            criteria_phrases = self.read_criteria(excel_path)\n",
    "            if not criteria_phrases:\n",
    "                self.logger.error(\"No criteria found. Exiting...\")\n",
    "                return\n",
    "            \n",
    "            # Get list of PDF files\n",
    "            pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "            if not pdf_files:\n",
    "                self.logger.error(\"No PDF files found in the specified folder.\")\n",
    "                return\n",
    "            \n",
    "            # Process each PDF with progress bar\n",
    "            all_results = []\n",
    "            for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "                self.logger.info(f\"Processing {pdf_file}...\")\n",
    "                pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "                \n",
    "                # Analyze PDF\n",
    "                results = self.analyze_single_pdf(pdf_path, criteria_phrases)  ######################################################\n",
    "                results['PDF File'] = pdf_file\n",
    "                all_results.append(results)\n",
    "            \n",
    "            # Save results\n",
    "            try:\n",
    "                results_df = pd.DataFrame(all_results)\n",
    "                results_df.to_excel(output_path, index=False)\n",
    "                self.logger.info(f\"Results saved to {output_path}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error saving results: {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in multiple PDF analysis: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.65)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/words.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_analysis_results.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "    analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa575c6-4ca8-4e31-ae4d-4e89592aaa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"Punkt is installed correctly!\")\n",
    "except LookupError:\n",
    "    print(\"Punkt is missing. Downloading now...\")\n",
    "    nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e7f8b-aeb5-4440-a832-2b472a9d635e",
   "metadata": {},
   "source": [
    "List of fixture:\n",
    "- Overlap 130 words, batch size optimal 32\n",
    "- Read file by blocks\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef01e8ea-e735-428f-b3d6-63b0859d2d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.74):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "        # NEW: Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()  # Changed variable name\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords\n",
    "            \"bị\", \"bởi\", \"cả\", \"các\", \"cái\", \"cần\", \"càng\", \"chỉ\", \"chiếc\", \"cho\", \"chứ\", \"chưa\", \"chuyện\", \n",
    "            \"có\", \"có_thể\", \"cứ\", \"của\", \"cùng\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đến_nỗi\", \"đều\", \"điều\", \n",
    "            \"do\", \"đó\", \"được\", \"dưới\", \"gì\", \"khi\", \"không\", \"là\", \"lại\", \"lên\", \"lúc\", \"mà\", \"mỗi\", \n",
    "            \"một_cách\", \"này\", \"nên\", \"nếu\", \"ngay\", \"nhiều\", \"như\", \"nhưng\", \"những\", \"nơi\", \"nữa\", \"phải\", \n",
    "            \"qua\", \"ra\", \"rằng\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"so\", \"sự\", \"tại\", \"theo\", \"thì\", \"trên\", \n",
    "            \"trước\", \"từ\", \"từng\", \"và\", \"vẫn\", \"vào\", \"vậy\", \"vì\", \"việc\", \"với\", \"vừa\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "    \n",
    "    def segment_words(self, text):\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        # Use segmentRawSentences to process the text\n",
    "        segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "        return segmented_text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Vietnamese characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        # Clean text (unchanged)\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords (unchanged)\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Use vws for word segmentation instead of simple split\n",
    "        segmented_text = self.segment_words(text)\n",
    "        \n",
    "        # Return segment words\n",
    "        return segmented_text\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract and preprocess text from PDF file\"\"\"\n",
    "        full_text = []\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text = \"\"\n",
    "                # Get text blocks with their positions\n",
    "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                for block in blocks:\n",
    "                    if \"lines\" in block:\n",
    "                        for line in block[\"lines\"]:\n",
    "                            for span in line[\"spans\"]:\n",
    "                                text += span[\"text\"] + \" \"\n",
    "                            text += \"\"\n",
    "                # Process the text for this page\n",
    "                processed_text = self.preprocess_text(text)\n",
    "                full_text.append(processed_text)\n",
    "        return ' '.join(full_text)\n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=32):\n",
    "        \"\"\"Get embeddings in batches\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            processed_batch = [self.preprocess_text(text) for text in batch]\n",
    "            \n",
    "            encoded = self.pho_tokenizer(processed_batch, \n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            \n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def read_criteria(self, excel_path):\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "\n",
    "    def analyze_single_pdf(self, pdf_path, criteria_phrases):\n",
    "        \"\"\"Analyze a single PDF file against criteria phrases\"\"\"\n",
    "        # Extract and preprocess text\n",
    "        pdf_text = self.extract_text_from_pdf(pdf_path)\n",
    "        if not pdf_text:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Split into overlapping chunks\n",
    "        words = pdf_text.split()\n",
    "        chunk_size = 1000\n",
    "        overlap_size = 130  \n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size - overlap_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        if not chunks:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Get embeddings\n",
    "        try:\n",
    "            criteria_embeddings = self.get_embeddings(criteria_phrases)\n",
    "            chunk_embeddings = self.get_embeddings(chunks)\n",
    "            \n",
    "            # Ensure proper shapes\n",
    "            if len(criteria_embeddings.shape) == 1:\n",
    "                criteria_embeddings = criteria_embeddings.reshape(1, -1)\n",
    "            if len(chunk_embeddings.shape) == 1:\n",
    "                chunk_embeddings = chunk_embeddings.reshape(1, -1)\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = cosine_similarity(criteria_embeddings, chunk_embeddings)\n",
    "            \n",
    "            # Initialize and update results\n",
    "            results = {}\n",
    "            for i, phrase in enumerate(criteria_phrases):\n",
    "                results[phrase] = 1 if np.max(similarities[i]) >= self.similarity_threshold else 0\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in analysis: {e}\")\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "\n",
    "    \n",
    "    def analyze_multiple_pdfs(self, pdf_folder, excel_path, output_path):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        # Read criteria\n",
    "        criteria_phrases = self.read_criteria(excel_path)\n",
    "        if not criteria_phrases:\n",
    "            print(\"No criteria found. Exiting...\")\n",
    "            return\n",
    "        \n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the specified folder.\")\n",
    "            return\n",
    "        \n",
    "        # Process each PDF\n",
    "        all_results = []\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            \n",
    "            # Analyze PDF\n",
    "            results = self.analyze_single_pdf(pdf_path, criteria_phrases)\n",
    "            results['PDF File'] = pdf_file\n",
    "            all_results.append(results)\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            results_df.to_excel(output_path, index=False)\n",
    "            print(f\"Results saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "            \n",
    "#Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.74)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/words.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_analysis_results.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e76a6ccc-c5ee-4de4-8e98-b1977f0c0d4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing aaa-bao-cao-phat-trien-ben-vung-nam-2022-0.pdf...\n",
      "Processing ACB ESG Report 2022 (Final).pdf...\n",
      "Processing BC-Ben-vung-nam-2019.pdf...\n",
      "Results saved to esg_analysis_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.8):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "        # NEW: Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()  # Changed variable name\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords\n",
    "            \"bị\", \"bởi\", \"cả\", \"các\", \"cái\", \"cần\", \"càng\", \"chỉ\", \"chiếc\", \"cho\", \"chứ\", \"chưa\", \"chuyện\", \n",
    "            \"có\", \"có_thể\", \"cứ\", \"của\", \"cùng\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đến_nỗi\", \"đều\", \"điều\", \n",
    "            \"do\", \"đó\", \"được\", \"dưới\", \"gì\", \"khi\", \"không\", \"là\", \"lại\", \"lên\", \"lúc\", \"mà\", \"mỗi\", \n",
    "            \"một_cách\", \"này\", \"nên\", \"nếu\", \"ngay\", \"nhiều\", \"như\", \"nhưng\", \"những\", \"nơi\", \"nữa\", \"phải\", \n",
    "            \"qua\", \"ra\", \"rằng\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"so\", \"sự\", \"tại\", \"theo\", \"thì\", \"trên\", \n",
    "            \"trước\", \"từ\", \"từng\", \"và\", \"vẫn\", \"vào\", \"vậy\", \"vì\", \"việc\", \"với\", \"vừa\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "    \n",
    "    def segment_words(self, text):\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        # Use segmentRawSentences to process the text\n",
    "        segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "        return segmented_text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Vietnamese characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        # Clean text (unchanged)\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords (unchanged)\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Use vws for word segmentation instead of simple split\n",
    "        segmented_text = self.segment_words(text)\n",
    "        \n",
    "        # Return segment words\n",
    "        return segmented_text\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract and preprocess text from PDF file\"\"\"\n",
    "        full_text = []\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text = \"\"\n",
    "                # Get text blocks with their positions\n",
    "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                for block in blocks:\n",
    "                    if \"lines\" in block:\n",
    "                        for line in block[\"lines\"]:\n",
    "                            for span in line[\"spans\"]:\n",
    "                                text += span[\"text\"] + \" \"\n",
    "                # Extract raw text from pdf\n",
    "                full_text.append(text)\n",
    "        return ' '.join(full_text)\n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=32):\n",
    "        \"\"\"Get embeddings in batches\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            \n",
    "            encoded = self.pho_tokenizer(batch, \n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            \n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def read_criteria(self, excel_path):\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()           # A list of criteria is collected\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]      # Process 1 item per time in a list of criteria\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "\n",
    "    def chunk_into_sentences(self, text):\n",
    "        \"\"\"Chunk the text into sentences based on delimiters (. ? ! ; ...).\"\"\"\n",
    "        sentence_endings = re.compile(r'([.!?;]{1,3})')  # Regex to match sentence-ending delimiters\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        sentences = [sentences[i].strip() + (sentences[i + 1] if i + 1 < len(sentences) else '')\n",
    "                     for i in range(0, len(sentences), 2)]  # Only take sentences (even index)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "    def analyze_single_pdf(self, pdf_path, criteria_phrases):\n",
    "        \"\"\"Analyze a single PDF file against criteria phrases\"\"\"\n",
    "        # Extract raw pdf text\n",
    "        pdf_text = self.extract_text_from_pdf(pdf_path)      # full pdf file, raw and original\n",
    "        if not pdf_text:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Split into overlapping chunks\n",
    "        \n",
    "        sentences = self.chunk_into_sentences(pdf_text)        # a list contain X items aligning with X sentences in the pdf file\n",
    "        if not sentences:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "\n",
    "        # Preprocess_text so it is cleaned\n",
    "        processed_text = [self.preprocess_text(senten) for senten in sentences]        # Process one item, in the list of X items, per time\n",
    "        \n",
    "        # Get embeddings\n",
    "        try:\n",
    "            criteria_embeddings = self.get_embeddings(criteria_phrases)\n",
    "            chunk_embeddings = self.get_embeddings(processed_text)\n",
    "            \n",
    "            # Ensure proper shapes\n",
    "            if len(criteria_embeddings.shape) == 1:\n",
    "                criteria_embeddings = criteria_embeddings.reshape(1, -1)\n",
    "            if len(chunk_embeddings.shape) == 1:\n",
    "                chunk_embeddings = chunk_embeddings.reshape(1, -1)\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = cosine_similarity(criteria_embeddings, chunk_embeddings)\n",
    "            \n",
    "            # Initialize and update results\n",
    "            results = {}\n",
    "            for i, phrase in enumerate(criteria_phrases):\n",
    "                results[phrase] = 1 if np.max(similarities[i]) >= self.similarity_threshold else 0\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in analysis: {e}\")\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "\n",
    "    \n",
    "    def analyze_multiple_pdfs(self, pdf_folder, excel_path, output_path):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        # Read criteria\n",
    "        criteria_phrases = self.read_criteria(excel_path)\n",
    "        if not criteria_phrases:\n",
    "            print(\"No criteria found. Exiting...\")\n",
    "            return\n",
    "        \n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the specified folder.\")\n",
    "            return\n",
    "        \n",
    "        # Process each PDF\n",
    "        all_results = []\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            \n",
    "            # Analyze PDF\n",
    "            results = self.analyze_single_pdf(pdf_path, criteria_phrases)\n",
    "            results['PDF File'] = pdf_file\n",
    "            all_results.append(results)\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            results_df.to_excel(output_path, index=False)\n",
    "            print(f\"Results saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "            \n",
    "#Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.8)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/words.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_analysis_results.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1c3faaf-4fb2-4a13-8184-b8fce96688bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing aaa-bao-cao-phat-trien-ben-vung-nam-2022-0.pdf...\n",
      "Processing ACB ESG Report 2022 (Final).pdf...\n",
      "Processing BC-Ben-vung-nam-2019.pdf...\n",
      "Results saved to esg_analysis_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.8):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "        # NEW: Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()  # Changed variable name\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords\n",
    "            \"bị\", \"bởi\", \"cả\", \"các\", \"cái\", \"cần\", \"càng\", \"chỉ\", \"chiếc\", \"cho\", \"chứ\", \"chưa\", \"chuyện\", \n",
    "            \"có\", \"có_thể\", \"cứ\", \"của\", \"cùng\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đến_nỗi\", \"đều\", \"điều\", \n",
    "            \"do\", \"đó\", \"được\", \"dưới\", \"gì\", \"khi\", \"không\", \"là\", \"lại\", \"lên\", \"lúc\", \"mà\", \"mỗi\", \n",
    "            \"một_cách\", \"này\", \"nên\", \"nếu\", \"ngay\", \"nhiều\", \"như\", \"nhưng\", \"những\", \"nơi\", \"nữa\", \"phải\", \n",
    "            \"qua\", \"ra\", \"rằng\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"so\", \"sự\", \"tại\", \"theo\", \"thì\", \"trên\", \n",
    "            \"trước\", \"từ\", \"từng\", \"và\", \"vẫn\", \"vào\", \"vậy\", \"vì\", \"việc\", \"với\", \"vừa\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "    \n",
    "    def segment_words(self, text):\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        # Use segmentRawSentences to process the text\n",
    "        segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "        return segmented_text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Vietnamese characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        # Clean text (unchanged)\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords (unchanged)\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Use vws for word segmentation instead of simple split\n",
    "        segmented_text = self.segment_words(text)\n",
    "        \n",
    "        # Return segment words\n",
    "        return segmented_text\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract and preprocess text from PDF file\"\"\"\n",
    "        full_text = []\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text = \"\"\n",
    "                # Get text blocks with their positions\n",
    "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                for block in blocks:\n",
    "                    if \"lines\" in block:\n",
    "                        for line in block[\"lines\"]:\n",
    "                            for span in line[\"spans\"]:\n",
    "                                text += span[\"text\"] + \" \"\n",
    "                # Extract raw text from pdf\n",
    "                full_text.append(text)\n",
    "        return ' '.join(full_text)\n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=1):\n",
    "        \"\"\"Get embeddings in batches\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            \n",
    "            encoded = self.pho_tokenizer(batch, \n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            \n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def read_criteria(self, excel_path):\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()           # A list of criteria is collected\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]      # Process 1 item per time in a list of criteria\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "\n",
    "    def chunk_into_sentences(self, text):\n",
    "        \"\"\"Chunk the text into sentences based on delimiters (. ? ! ; ...).\"\"\"\n",
    "        sentence_endings = re.compile(r'([.!?;]{1,3})')  # Regex to match sentence-ending delimiters\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        sentences = [sentences[i].strip() + (sentences[i + 1] if i + 1 < len(sentences) else '')\n",
    "                     for i in range(0, len(sentences), 2)]  # Only take sentences (even index)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "    def analyze_single_pdf(self, pdf_path, criteria_phrases):\n",
    "        \"\"\"Analyze a single PDF file against criteria phrases\"\"\"\n",
    "        # Extract raw pdf text\n",
    "        pdf_text = self.extract_text_from_pdf(pdf_path)      # full pdf file, raw and original\n",
    "        if not pdf_text:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Split into overlapping chunks\n",
    "        \n",
    "        sentences = self.chunk_into_sentences(pdf_text)        # a list contain X items aligning with X sentences in the pdf file\n",
    "        if not sentences:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "\n",
    "        # Preprocess_text so it is cleaned\n",
    "        processed_text = [self.preprocess_text(senten) for senten in sentences]        # Process one item, in the list of X items, per time\n",
    "        \n",
    "        # Get embeddings\n",
    "        try:\n",
    "            criteria_embeddings = [self.get_embeddings(phrase) for phrase in criteria_phrases]\n",
    "            \n",
    "            results = {phrase: 0 for phrase in criteria_phrases}\n",
    "            for sentence in processed_text:\n",
    "                sentence_embedding = self.get_embeddings(sentence)\n",
    "                if len(sentence_embedding.shape) == 1:\n",
    "                    sentence_embedding = sentence_embedding.reshape(1, -1)\n",
    "                for i, criterion_embedding in enumerate(criteria_embeddings):\n",
    "                    similarity = cosine_similarity(sentence_embedding, criterion_embedding)[0][0]\n",
    "                    if similarity >= self.similarity_threshold:\n",
    "                        results[criteria_phrases[i]] = 1\n",
    "                    \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in analysis: {e}\")\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "\n",
    "    \n",
    "    def analyze_multiple_pdfs(self, pdf_folder, excel_path, output_path):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        # Read criteria\n",
    "        criteria_phrases = self.read_criteria(excel_path)\n",
    "        if not criteria_phrases:\n",
    "            print(\"No criteria found. Exiting...\")\n",
    "            return\n",
    "        \n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the specified folder.\")\n",
    "            return\n",
    "        \n",
    "        # Process each PDF\n",
    "        all_results = []\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            \n",
    "            # Analyze PDF\n",
    "            results = self.analyze_single_pdf(pdf_path, criteria_phrases)\n",
    "            results['PDF File'] = pdf_file\n",
    "            all_results.append(results)\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            results_df.to_excel(output_path, index=False)\n",
    "            print(f\"Results saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "            \n",
    "#Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.8)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/words.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_analysis_results.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5bc68ed-9293-46e6-89d3-d0dc03e980bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TH_milk.pdf...\n",
      "Results saved to esg_analysis_results_0.6.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.67):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "        # NEW: Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()  # Changed variable name\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords\n",
    "            \"bị\", \"bởi\", \"cả\", \"các\", \"cái\", \"cần\", \"càng\", \"chỉ\", \"chiếc\", \"cho\", \"chứ\", \"chưa\", \"chuyện\", \n",
    "            \"có\", \"có_thể\", \"cứ\", \"của\", \"cùng\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đến_nỗi\", \"đều\", \"điều\", \n",
    "            \"do\", \"đó\", \"được\", \"dưới\", \"gì\", \"khi\", \"không\", \"là\", \"lại\", \"lên\", \"lúc\", \"mà\", \"mỗi\", \n",
    "            \"một_cách\", \"này\", \"nên\", \"nếu\", \"ngay\", \"nhiều\", \"như\", \"nhưng\", \"những\", \"nơi\", \"nữa\", \"phải\", \n",
    "            \"qua\", \"ra\", \"rằng\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"so\", \"sự\", \"tại\", \"theo\", \"thì\", \"trên\", \n",
    "            \"trước\", \"từ\", \"từng\", \"và\", \"vẫn\", \"vào\", \"vậy\", \"vì\", \"việc\", \"với\", \"vừa\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "    \n",
    "    def segment_words(self, text):\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        # Use segmentRawSentences to process the text\n",
    "        segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "        return segmented_text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Vietnamese characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        # Clean text (unchanged)\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords (unchanged)\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Use vws for word segmentation instead of simple split\n",
    "        segmented_text = self.segment_words(text)\n",
    "        \n",
    "        # Return segment words\n",
    "        return segmented_text\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract and preprocess text from PDF file\"\"\"\n",
    "        full_text = []\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text = \"\"\n",
    "                # Get text blocks with their positions\n",
    "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                for block in blocks:\n",
    "                    if \"lines\" in block:\n",
    "                        for line in block[\"lines\"]:\n",
    "                            for span in line[\"spans\"]:\n",
    "                                text += span[\"text\"] + \" \"\n",
    "                # Extract raw text from pdf\n",
    "                full_text.append(text)\n",
    "        return ' '.join(full_text)\n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=1):\n",
    "        \"\"\"Get embeddings in batches\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            \n",
    "            encoded = self.pho_tokenizer(batch, \n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            \n",
    "                # Use mean pooling instead of just [CLS] token\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            batch_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            \n",
    "\n",
    "            # L2 normalization for better cosine similarity\n",
    "            batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "            batch_embeddings = batch_embeddings.numpy()\n",
    "            \n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def read_criteria(self, excel_path):\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()           # A list of criteria is collected\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]      # Process 1 item per time in a list of criteria\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "\n",
    "    def chunk_into_sentences(self, text):\n",
    "        \"\"\"Chunk the text into sentences based on delimiters (. ? ! ; ...).\"\"\"\n",
    "        sentence_endings = re.compile(r'([.!?;]{1,3})')  # Regex to match sentence-ending delimiters\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        sentences = [sentences[i].strip() + (sentences[i + 1] if i + 1 < len(sentences) else '')\n",
    "                     for i in range(0, len(sentences), 2)]  # Only take sentences (even index)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "    def analyze_single_pdf(self, pdf_path, criteria_phrases):\n",
    "        \"\"\"Analyze a single PDF file against criteria phrases\"\"\"\n",
    "        # Extract raw pdf text\n",
    "        pdf_text = self.extract_text_from_pdf(pdf_path)      # full pdf file, raw and original\n",
    "        if not pdf_text:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Split into overlapping chunks\n",
    "        \n",
    "        sentences = self.chunk_into_sentences(pdf_text)        # a list contain X items aligning with X sentences in the pdf file\n",
    "        if not sentences:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "\n",
    "        # Preprocess_text so it is cleaned\n",
    "        processed_text = [self.preprocess_text(senten) for senten in sentences]        # Process one item, in the list of X items, per time\n",
    "\n",
    "        # Get embeddings\n",
    "        try:\n",
    "            criteria_embeddings = [self.get_embeddings(phrase) for phrase in criteria_phrases]\n",
    "            \n",
    "            results = {phrase: 0 for phrase in criteria_phrases}\n",
    "            for sentence in processed_text:\n",
    "                sentence_embedding = self.get_embeddings(sentence)\n",
    "                if len(sentence_embedding.shape) == 1:\n",
    "                    sentence_embedding = sentence_embedding.reshape(1, -1)\n",
    "                for i, criterion_embedding in enumerate(criteria_embeddings):\n",
    "                    similarity = cosine_similarity(sentence_embedding, criterion_embedding)[0][0]\n",
    "                    if similarity >= self.similarity_threshold:\n",
    "                        results[criteria_phrases[i]] = 1\n",
    "                    \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in analysis: {e}\")\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "\n",
    "    \n",
    "    def analyze_multiple_pdfs(self, pdf_folder, excel_path, output_path):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        # Read criteria\n",
    "        criteria_phrases = self.read_criteria(excel_path)\n",
    "        if not criteria_phrases:\n",
    "            print(\"No criteria found. Exiting...\")\n",
    "            return\n",
    "        \n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the specified folder.\")\n",
    "            return\n",
    "        \n",
    "        # Process each PDF\n",
    "        all_results = []\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            \n",
    "            # Analyze PDF\n",
    "            results = self.analyze_single_pdf(pdf_path, criteria_phrases)\n",
    "            results['PDF File'] = pdf_file\n",
    "            all_results.append(results)\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            results_df.to_excel(output_path, index=False)\n",
    "            print(f\"Results saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "            \n",
    "#Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.67)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/words_test.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_analysis_results_0.6.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65a98b10-9572-4841-93e7-07c4d640689e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AME_Baocaothuongnien_2023-pages-2.pdf...\n",
      "Processing TH_milk.pdf...\n",
      "Results saved to esg_analysis_results_exactmatch_067.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.67):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "        # NEW: Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()  # Changed variable name\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords\n",
    "            \"bị\", \"bởi\", \"cả\", \"các\", \"cái\", \"cần\", \"càng\", \"chỉ\", \"chiếc\", \"cho\", \"chứ\", \"chưa\", \"chuyện\", \n",
    "            \"có\", \"có_thể\", \"cứ\", \"của\", \"cùng\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đến_nỗi\", \"đều\", \"điều\", \n",
    "            \"do\", \"đó\", \"được\", \"dưới\", \"gì\", \"khi\", \"không\", \"là\", \"lại\", \"lên\", \"lúc\", \"mà\", \"mỗi\", \n",
    "            \"một_cách\", \"này\", \"nên\", \"nếu\", \"ngay\", \"nhiều\", \"như\", \"nhưng\", \"những\", \"nơi\", \"nữa\", \"phải\", \n",
    "            \"qua\", \"ra\", \"rằng\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"so\", \"sự\", \"tại\", \"theo\", \"thì\", \"trên\", \n",
    "            \"trước\", \"từ\", \"từng\", \"và\", \"vẫn\", \"vào\", \"vậy\", \"vì\", \"việc\", \"với\", \"vừa\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "    \n",
    "    def segment_words(self, text):\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        # Use segmentRawSentences to process the text\n",
    "        segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "        return segmented_text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Vietnamese characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        # Clean text (unchanged)\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords (unchanged)\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Use vws for word segmentation instead of simple split\n",
    "        segmented_text = self.segment_words(text)\n",
    "        \n",
    "        # Return segment words\n",
    "        return segmented_text\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract and preprocess text from PDF file\"\"\"\n",
    "        full_text = []\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text = \"\"\n",
    "                # Get text blocks with their positions\n",
    "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                for block in blocks:\n",
    "                    if \"lines\" in block:\n",
    "                        for line in block[\"lines\"]:\n",
    "                            for span in line[\"spans\"]:\n",
    "                                text += span[\"text\"] + \" \"\n",
    "                # Extract raw text from pdf\n",
    "                full_text.append(text)\n",
    "        return ' '.join(full_text)\n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=1):\n",
    "        \"\"\"Get embeddings in batches\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            \n",
    "            encoded = self.pho_tokenizer(batch, \n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            \n",
    "                # Use mean pooling instead of just [CLS] token\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            batch_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            \n",
    "\n",
    "            # L2 normalization for better cosine similarity\n",
    "            batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "            batch_embeddings = batch_embeddings.numpy()\n",
    "            \n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def read_criteria(self, excel_path):\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()           # A list of criteria is collected\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]      # Process 1 item per time in a list of criteria\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "\n",
    "    def chunk_into_sentences(self, text):\n",
    "        \"\"\"Chunk the text into sentences based on delimiters (. ? ! ; ...).\"\"\"\n",
    "        sentence_endings = re.compile(r'([.!?;]{1,3})')  # Regex to match sentence-ending delimiters\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        sentences = [sentences[i].strip() + (sentences[i + 1] if i + 1 < len(sentences) else '')\n",
    "                     for i in range(0, len(sentences), 2)]  # Only take sentences (even index)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "    def analyze_single_pdf(self, pdf_path, criteria_phrases):\n",
    "        \"\"\"Analyze a single PDF file against criteria phrases\"\"\"\n",
    "        # Extract raw pdf text\n",
    "        pdf_text = self.extract_text_from_pdf(pdf_path)      # full pdf file, raw and original\n",
    "        if not pdf_text:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Initialize results dictionary\n",
    "        results = {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # First, perform exact matching\n",
    "        pdf_text_lower = pdf_text.lower()\n",
    "        for phrase in criteria_phrases:\n",
    "            clean_phrase = self.clean_text(phrase).lower()\n",
    "            if clean_phrase in pdf_text_lower:\n",
    "                results[phrase] = 1\n",
    "        \n",
    "        # Get list of unmatched phrases for semantic analysis\n",
    "        unmatched_phrases = [phrase for phrase in criteria_phrases if results[phrase] == 0]\n",
    "        \n",
    "        # If all phrases were matched exactly, return results\n",
    "        if not unmatched_phrases:\n",
    "            return results\n",
    "            \n",
    "        # Otherwise, proceed with semantic similarity analysis for unmatched phrases\n",
    "        sentences = self.chunk_into_sentences(pdf_text)\n",
    "        if not sentences:\n",
    "            return results\n",
    "    \n",
    "        # Preprocess_text so it is cleaned\n",
    "        processed_text = [self.preprocess_text(senten) for senten in sentences]\n",
    "        \n",
    "        # Get embeddings only for unmatched phrases\n",
    "        try:\n",
    "            unmatched_embeddings = [self.get_embeddings(phrase) for phrase in unmatched_phrases]\n",
    "            \n",
    "            # Check semantic similarity only for unmatched phrases\n",
    "            for sentence in processed_text:\n",
    "                sentence_embedding = self.get_embeddings(sentence)\n",
    "                if len(sentence_embedding.shape) == 1:\n",
    "                    sentence_embedding = sentence_embedding.reshape(1, -1)\n",
    "                for i, criterion_embedding in enumerate(unmatched_embeddings):\n",
    "                    similarity = cosine_similarity(sentence_embedding, criterion_embedding)[0][0]\n",
    "                    if similarity >= self.similarity_threshold:\n",
    "                        results[unmatched_phrases[i]] = 1\n",
    "                \n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic analysis: {e}\")\n",
    "            return results\n",
    "\n",
    "    \n",
    "    def analyze_multiple_pdfs(self, pdf_folder, excel_path, output_path):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        # Read criteria\n",
    "        criteria_phrases = self.read_criteria(excel_path)\n",
    "        if not criteria_phrases:\n",
    "            print(\"No criteria found. Exiting...\")\n",
    "            return\n",
    "        \n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the specified folder.\")\n",
    "            return\n",
    "        \n",
    "        # Process each PDF\n",
    "        all_results = []\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            \n",
    "            # Analyze PDF\n",
    "            results = self.analyze_single_pdf(pdf_path, criteria_phrases)\n",
    "            results['PDF File'] = pdf_file\n",
    "            all_results.append(results)\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            results_df.to_excel(output_path, index=False)\n",
    "            print(f\"Results saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "            \n",
    "#Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.67)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/words_test.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_analysis_results_exactmatch_067.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a043b0b-a872-482a-b05d-333060bb6c35",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Using cached pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pdf2image\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from pytesseract) (24.1)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from pytesseract) (10.4.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Using cached pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Using cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "Installing collected packages: pytesseract, pdf2image, opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86 pdf2image-1.17.0 pytesseract-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract pdf2image opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a3541b-1af5-4232-9866-715ca35e7d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting transformers==4.50.3\n",
      "  Using cached transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from transformers==4.50.3) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers==4.50.3)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from transformers==4.50.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from transformers==4.50.3) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from transformers==4.50.3) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from transformers==4.50.3) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from transformers==4.50.3) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.50.3)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.50.3)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from transformers==4.50.3) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.50.3) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from requests->transformers==4.50.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from requests->transformers==4.50.3) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from requests->transformers==4.50.3) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\downloads\\anaconda3\\lib\\site-packages (from requests->transformers==4.50.3) (2024.8.30)\n",
      "Using cached transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "Using cached torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: sympy, safetensors, torch, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed huggingface-hub-0.30.2 safetensors-0.5.3 sympy-1.13.1 tokenizers-0.21.1 torch-2.6.0 transformers-4.50.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers==4.50.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b01092-2cff-473d-af7e-c0f8edefd2f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Using cached pymupdf-1.25.5-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Using cached pymupdf-1.25.5-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.25.5\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed42d3-e6f9-4f8b-ad73-eb9242959d47",
   "metadata": {},
   "source": [
    "## pip instALL CHO CODE 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8c22571-b3e2-48f8-982b-99a14215420d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyocr\n",
      "  Downloading pyocr-0.8.5-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\vnc\\downloads\\anaconda jpt\\lib\\site-packages (from pyocr) (10.4.0)\n",
      "Downloading pyocr-0.8.5-py3-none-any.whl (40 kB)\n",
      "Installing collected packages: pyocr\n",
      "Successfully installed pyocr-0.8.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033addca-28bd-47c4-b4ae-fb54d5b993c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: C:\\Users\\HP\\Downloads\\Tesseract-OCR\\tesseract.exe is not installed or it's not in your PATH. See README file for more information.\n",
      "Please make sure Tesseract is properly installed at: C:\\Users\\HP\\Downloads\\Tesseract-OCR\\tesseract.exe\n"
     ]
    }
   ],
   "source": [
    "# Set the path to tesseract executable\n",
    "tesseract_path = os.path.join(os.path.expanduser('~'), 'Downloads', 'Tesseract-OCR', 'tesseract.exe')\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_path\n",
    "\n",
    "# Verify Tesseract installation\n",
    "try:\n",
    "    print(f\"Tesseract version: {pytesseract.get_tesseract_version()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please make sure Tesseract is properly installed at:\", tesseract_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2de7b89-3f59-4538-9989-80c2165e37ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Test Result: Xin ch??o\n",
      "T??i !?? m??y t??nh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_ocr_with_vietnamese():\n",
    "    # Create a test image with Vietnamese text\n",
    "    img = np.zeros((200, 400, 3), dtype=np.uint8)\n",
    "    cv2.putText(img, \"Xin chào\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(img, \"Tôi là máy tính\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    # Try OCR with Vietnamese\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(img, lang='vie', config='--psm 6')\n",
    "        print(\"OCR Test Result:\", text)\n",
    "    except Exception as e:\n",
    "        print(\"OCR Test Failed:\", e)\n",
    "\n",
    "# Run the test\n",
    "test_ocr_with_vietnamese()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d93ccdf6-ae15-4ebe-b18b-11141d863317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesseract version: 5.5.0.20241111\n",
      "\n",
      "Checking Vietnamese language pack:\n",
      "Path to vie.traineddata: C:\\Users\\VNC\\Downloads\\Tesseract-OCR\\tessdata\\vie.traineddata\n",
      "File exists: True\n",
      "File size: 531275 bytes\n",
      "\n",
      "Available languages:\n",
      "['eng', 'osd', 'vie']\n"
     ]
    }
   ],
   "source": [
    "def verify_tesseract_setup():\n",
    "    # Check Tesseract version\n",
    "    print(\"Tesseract version:\", pytesseract.get_tesseract_version())\n",
    "    \n",
    "    # Check if Vietnamese language pack is installed\n",
    "    tessdata_dir = os.path.join(os.path.expanduser('~'), 'Downloads', 'Tesseract-OCR', 'tessdata')\n",
    "    vie_data_path = os.path.join(tessdata_dir, 'vie.traineddata')\n",
    "    \n",
    "    print(\"\\nChecking Vietnamese language pack:\")\n",
    "    print(\"Path to vie.traineddata:\", vie_data_path)\n",
    "    print(\"File exists:\", os.path.exists(vie_data_path))\n",
    "    \n",
    "    if os.path.exists(vie_data_path):\n",
    "        print(\"File size:\", os.path.getsize(vie_data_path), \"bytes\")\n",
    "    \n",
    "    # List all available languages\n",
    "    print(\"\\nAvailable languages:\")\n",
    "    try:\n",
    "        langs = pytesseract.get_languages()\n",
    "        print(langs)\n",
    "    except Exception as e:\n",
    "        print(\"Error getting languages:\", e)\n",
    "\n",
    "# Run the verification\n",
    "verify_tesseract_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "858731f3-6520-45ee-b899-60103cf8b017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF with 3 pages\n",
      "\n",
      "Processing page 1\n",
      "Found 1 images in page 1\n",
      "\n",
      "Results for image 0 on page 1:\n",
      "\n",
      "Using --psm 6:\n",
      "CÔNG TY CỎ PHẢN ALPHANAM E&C\n",
      "\n",
      "Tâng 3, sô 108 Nguyên Trãi, Phường Thượng Đình, quận Thanh Xuân, Hà Nội, Việt Nam\n",
      "Các cuộc họp của Hội đồng quản trị trong năm, Tổng Giám đốc và các thành viên được mời\n",
      "tham dự và báo cáo, đông thời thông báo đây đủ các thông tin vê nội dung cuộc họp đề đảm\n",
      "báo Tổng Giám đốc thực hiện nghiêm túc các nhiệm vụ và quyền hạn theo đúng quy định.\n",
      "Trong năm, Hội đồng quản trị không ghi nhận bất thường trong các hoạt động quản lý và\n",
      "điều hành của Tổng Giám đốc và các cán bộ quản lý.\n",
      "\n",
      "3. Các kế hoạch, định hướng của Hội đồng quản trị :\n",
      "\n",
      "- __ Phát triển ôn định và bền vững các lĩnh vực cốt lõi, truyền thông.\n",
      "\n",
      "- _ Tim hiểu, tận dụng các cơ hội thị trường để mở rộng đầu tư vào các dự án chiến lược, có\n",
      "\n",
      "tiềm năng cao về số lượng và chất lượng cho Công ty đầu tư.\n",
      "\n",
      "- _ Chú trọng công tác phát triển nguồn nhân lực có chất lượng cao, phát triển đội ngũ quản lý,\n",
      "\n",
      "gắn trách nhiệm của nhân viên với lợi ích công việc.\n",
      "Vụ, Quản trị Công ty\n",
      "\n",
      "1. Hội đồng quản tr] :\n",
      "\n",
      "a. Thành viên và cơ cầu Hội đồng quản trị :\n",
      "\n",
      "Tỷ lệ sở hữu cỗ phần\n",
      "SIT Thành viên HĐỌT Chức vụ có quyền biểu quyết\n",
      "` \" 0\n",
      "\n",
      "b. Các tiểu ban của Hội đồng quản trị: Không có\n",
      "\n",
      "c. Hoạt động của Hội đồng quản trị:\n",
      "\n",
      "Thực hiện theo quy định tại Điều lệ Công ty cổ phần Alphanam E&C (“Công ty”), các Quy\n",
      "định, Quy chế quản lý nội bộ và Pháp luật hiện hành, trong năm 2023, Hội đồng quản trị và\n",
      "Ban điều hành Công ty đã tiễn hành 45 cuộc họp chung nhằm thống nhất một số nội dung\n",
      "liên quan đến tình hình sản xuất kinh đoanh của Công ty như sau:\n",
      "\n",
      "- _ Đôn đốc, kiểm tra, giám sát các hoạt động của Ban điều hành Công ty trong công tác triển\n",
      "khai hoạt động sản xuất kinh doanh và chỉ tiêu kế hoạch đã đề ra theo Nghị quyết của Đại\n",
      "hội đồng cổ đông 2023.\n",
      "\n",
      "- _ Thực hiện chế độ báo cáo định kỳ và bất thường (khi có phát sinh) của Ban điều hành cho\n",
      "HĐQT về tình hình kinh doanh và điều hành Công ty.\n",
      "\n",
      "-___ Giám sát, chỉ đạo công bố thông tin với mục tiêu đảm bảo tính minh bạch, kịp thời theo đúng\n",
      "quy định của pháp luật.\n",
      "\n",
      "- __ Giải quyết các vấn đề nhân sự.\n",
      "\n",
      "d. Hoạt động của các thành viên Hội đồng quản trị độc lập: Không có.\n",
      "\n",
      "e. _ Danh sách các thành viên Hội đồng quản trị có chứng chỉ đào tạo về quản trị công ty. Danh\n",
      "sách các thành viên hội đồng quản trị tham gia các chương trình về quản trị công ty trong\n",
      "năm: Không có.\n",
      "\n",
      "10\n",
      "\n",
      "Processing page 2\n",
      "Found 1 images in page 2\n",
      "\n",
      "Results for image 0 on page 2:\n",
      "\n",
      "Using --psm 6:\n",
      "CÔNG TY CÔ PHẢN ALPHANAM E&C\n",
      "Tâng 3, số 108 Nguyên Trãi, Phường Thượng Đình, quận Thanh Xuân, Hà Nội, Việt Nam\n",
      "2. Ban kiểm soát\n",
      "a. _ Thành viên và cơ cấu Ban kiểm soát:\n",
      "Tỷ lệ sở hữu cô phần có\n",
      "STT Họ và tên Chức vụ quyền biểu quyết\n",
      "b. Hoạt động của Ban kiểm soát\n",
      "- _ Đôn đốc, kiểm tra, giám sát các hoạt động của Ban điều hành Công ty trong công tác triển\n",
      "khai hoạt động sản xuất kinh doanh và chỉ tiêu kế hoạch đã để ra theo Nghị quyết của Đại\n",
      "hội đồng cỗ đông 2023.\n",
      "- _ Thực hiện chế độ báo cáo định kỳ và bất thường (khi có phát sinh) của Ban điều hành Công\n",
      "ty cho HĐQT về tình hình kinh doanh và điều hành Công ty.\n",
      "- — GIám sát, chỉ đạo công bổ thông tin với mục tiêu đảm bảo tính minh bạch, kịp thời theo\n",
      "đúng quy định của pháp luật.\n",
      "- _ Giải quyết các vấn đẻ nhân sự.\n",
      "3. Các giao dịch, thù lao và khoản lợi ích của Hội đồng quản trị, Ban điều hành và ban\n",
      "kiêm soát:\n",
      "a.. Lương, thưởng, thù lao, các khoản lợi ích: Hội đồng quản trị và Ban kiểm soát không hưởng\n",
      "thù lao.\n",
      "b. Giao dịch cô phiếu của cổ đông nội bộ trong năm: Không có\n",
      "c.. Hợp đồng hoặc giao dịch với cô đông nội bộ trong năm: Không có\n",
      "d. Đánh giá thực hiện các qui định về quản trị công ty: Hội đồng quản trị, Ban điều hành, Ban\n",
      "kiểm soát luôn nghiêm chỉnh chấp hành và thực hiện các qui định về quản trị công ty.\n",
      "VỊ. Báo cáo tài chính\n",
      "1. Ý kiến kiểm toán viên\n",
      "Theo ý kiến của chúng tôi, báo cáo tài chính đã phản ánh trung thực và hợp lý, trên các khía\n",
      "cạnh trọng yếu tình hình tài chính của Công ty Cô phần Alphanam E&C tại ngày 31 tháng\n",
      "12 năm 2023, cũng như kết quả hoạt động kinh doanh và tình hình lưu chuyển tiền tệ cho\n",
      "năm tài chính kết thúc cùng ngày, phù hợp với chuẩn mực kế toán, chế độ kế toán doanh\n",
      "nghiệp Việt Nam và các quy định pháp lý có liên quan đến việc lập và trình bày báo cáo tài\n",
      "chính.\n",
      "Hà Nội, ngày 30 tháng 03 năm 2024\n",
      "Công ty TNHH Dịch vụ Tư vấn Tài chính\n",
      "Kế toán và Kiểm toán Nam Việt - Chi\n",
      "nhanh phía Bắc\n",
      "II\n",
      "\n",
      "Processing page 3\n",
      "Found 1 images in page 3\n",
      "\n",
      "Results for image 0 on page 3:\n",
      "\n",
      "Using --psm 6:\n",
      "CÔNG TY CÓ PHẦN ALPHANAM E&C\n",
      "Tâng 3, sô 108 Nguyễn Trãi, Phường Thượng Đình, quận Thanh Xuân, Hà Nội, Việt Nam\n",
      "Giám đốc Kiểm toán viên\n",
      "Phạm Văn Cường Bùi Ngọc Hà\n",
      "Số Giấy CN ĐKHN kiểm toán: 2922-2024- Số Giấy CN ĐKHN kiểm toán: 0662-2023-\n",
      "152-1 152-1\n",
      "\n",
      "2. Báo cáo kiếm toán cho năm tài chính kết thúc ngày 31 tháng 12 năm 2023 của Công ty\n",
      "\n",
      "Cô phần Alphanam E&C được đính kèm ở cuối báo cáo này.\n",
      "Toàn văn báo cáo tài chính đã được kiểm toán cho năm tài chính kết thúc ngày 31 tháng 12\n",
      "năm 2023 của Công ty Cổ phần Alphanam E&C được công bố tại website Công ty theo\n",
      "\n",
      "đường link: Báo cáo tải chính năm 2023 (alphanamec.com.vn)\n",
      "Hà Nội, ngày 10 tháng 04 năm 2024\n",
      "NGƯỜI ĐẠI DIỆN PHÁP LUẬT\n",
      "ảng Giám đốc\n",
      "Sư voÀ\n",
      "sự CÔNG TY NÀÀ\n",
      "` &ỳ\n",
      "à TY =— “S5\n",
      "rương Thị Thu Hiên\n",
      "2l\n",
      "⁄\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "\n",
    "# Set your Tesseract path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\VNC\\Downloads\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def read_vietnamese_pdf(pdf_path):\n",
    "    # Open the PDF\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        print(f\"Processing PDF with {len(doc)} pages\")\n",
    "        \n",
    "        for page_num, page in enumerate(doc, 1):\n",
    "            print(f\"\\nProcessing page {page_num}\")\n",
    "            \n",
    "            # Get images from the page\n",
    "            image_list = page.get_images()\n",
    "            print(f\"Found {len(image_list)} images in page {page_num}\")\n",
    "            \n",
    "            for img_index, img in enumerate(image_list):\n",
    "                try:\n",
    "                    # Get image data\n",
    "                    xref = img[0]\n",
    "                    base_image = doc.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "                    \n",
    "                    # Convert to numpy array\n",
    "                    nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "                    img_np = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "                    \n",
    "                    # Preprocess the image\n",
    "                    # Convert to grayscale\n",
    "                    gray = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
    "                    \n",
    "                    # Apply thresholding\n",
    "                    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "                    \n",
    "                    # Save preprocessed image for debugging\n",
    "                    debug_path = f'preprocessed_page_{page_num}_img_{img_index}.png'\n",
    "                    cv2.imwrite(debug_path, thresh)\n",
    "                    \n",
    "                    # Try different PSM modes\n",
    "                    psm_modes = ['--psm 6']\n",
    "                    \n",
    "                    print(f\"\\nResults for image {img_index} on page {page_num}:\")\n",
    "                    for psm in psm_modes:\n",
    "                        text = pytesseract.image_to_string(\n",
    "                            thresh,\n",
    "                            lang='vie',\n",
    "                            config=psm\n",
    "                        )\n",
    "                        if text.strip():  # Only print if text was found\n",
    "                            print(f\"\\nUsing {psm}:\")\n",
    "                            print(text.strip())\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {img_index} on page {page_num}: {e}\")\n",
    "                    continue\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your PDF path\n",
    "    pdf_path = \"Downloads/AME_Baocaothuongnien_2023-pages-2.pdf\"  # Update this path\n",
    "    read_vietnamese_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64d639e-8499-494b-b239-db0aab3b85cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2012 - ACC.pdf...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 341\u001b[0m\n\u001b[0;32m    338\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mesg_analysis_results_exactmatch_imageread.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;66;03m# Run analysis\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m analyzer\u001b[38;5;241m.\u001b[39manalyze_multiple_pdfs(pdf_folder, excel_path, output_path)\n",
      "Cell \u001b[1;32mIn[1], line 319\u001b[0m, in \u001b[0;36mESGAnalyzer.analyze_multiple_pdfs\u001b[1;34m(self, pdf_folder, excel_path, output_path)\u001b[0m\n\u001b[0;32m    316\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdf_folder, pdf_file)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# Analyze PDF\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyze_single_pdf(pdf_path, criteria_phrases)\n\u001b[0;32m    320\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPDF File\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pdf_file\n\u001b[0;32m    321\u001b[0m all_results\u001b[38;5;241m.\u001b[39mappend(results)\n",
      "Cell \u001b[1;32mIn[1], line 248\u001b[0m, in \u001b[0;36mESGAnalyzer.analyze_single_pdf\u001b[1;34m(self, pdf_path, criteria_phrases)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Analyze a single PDF file against criteria phrases\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# Extract raw pdf text\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m pdf_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_text_from_pdf(pdf_path)      \u001b[38;5;66;03m# full pdf file, raw and original\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pdf_text:\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {phrase: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m criteria_phrases}\n",
      "Cell \u001b[1;32mIn[1], line 144\u001b[0m, in \u001b[0;36mESGAnalyzer.extract_text_from_pdf\u001b[1;34m(self, pdf_path)\u001b[0m\n\u001b[0;32m    141\u001b[0m processed \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mmorphologyEx(deskewed, cv2\u001b[38;5;241m.\u001b[39mMORPH_CLOSE, kernel)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# === Tesseract OCR with layout preservation ===\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m d \u001b[38;5;241m=\u001b[39m pytesseract\u001b[38;5;241m.\u001b[39mimage_to_data(\n\u001b[0;32m    145\u001b[0m     processed,\n\u001b[0;32m    146\u001b[0m     lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvie\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    147\u001b[0m     config\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--oem 3 --psm 6\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    148\u001b[0m     output_type\u001b[38;5;241m=\u001b[39mpytesseract\u001b[38;5;241m.\u001b[39mOutput\u001b[38;5;241m.\u001b[39mDICT\n\u001b[0;32m    149\u001b[0m )\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Group words into structured text (block → line → word)\u001b[39;00m\n\u001b[0;32m    152\u001b[0m blocks \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))\n",
      "File \u001b[1;32m~\\Downloads\\Anaconda3\\Lib\\site-packages\\pytesseract\\pytesseract.py:596\u001b[0m, in \u001b[0;36mimage_to_data\u001b[1;34m(image, lang, config, nice, output_type, timeout, pandas_config)\u001b[0m\n\u001b[0;32m    593\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-c tessedit_create_tsv=1 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    594\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsv\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[1;32m--> 596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    597\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[0;32m    598\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDATAFRAME: \u001b[38;5;28;01mlambda\u001b[39;00m: get_pandas_output(\n\u001b[0;32m    599\u001b[0m         args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m],\n\u001b[0;32m    600\u001b[0m         pandas_config,\n\u001b[0;32m    601\u001b[0m     ),\n\u001b[0;32m    602\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: file_to_dict(run_and_get_output(\u001b[38;5;241m*\u001b[39margs), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    603\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs),\n\u001b[0;32m    604\u001b[0m }[output_type]()\n",
      "File \u001b[1;32m~\\Downloads\\Anaconda3\\Lib\\site-packages\\pytesseract\\pytesseract.py:602\u001b[0m, in \u001b[0;36mimage_to_data.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    593\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-c tessedit_create_tsv=1 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    594\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsv\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    597\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[0;32m    598\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDATAFRAME: \u001b[38;5;28;01mlambda\u001b[39;00m: get_pandas_output(\n\u001b[0;32m    599\u001b[0m         args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m],\n\u001b[0;32m    600\u001b[0m         pandas_config,\n\u001b[0;32m    601\u001b[0m     ),\n\u001b[1;32m--> 602\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: file_to_dict(run_and_get_output(\u001b[38;5;241m*\u001b[39margs), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    603\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs),\n\u001b[0;32m    604\u001b[0m }[output_type]()\n",
      "File \u001b[1;32m~\\Downloads\\Anaconda3\\Lib\\site-packages\\pytesseract\\pytesseract.py:352\u001b[0m, in \u001b[0;36mrun_and_get_output\u001b[1;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m save(image) \u001b[38;5;28;01mas\u001b[39;00m (temp_name, input_filename):\n\u001b[0;32m    342\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    343\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_filename\u001b[39m\u001b[38;5;124m'\u001b[39m: input_filename,\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m: temp_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[0;32m    350\u001b[0m     }\n\u001b[1;32m--> 352\u001b[0m     run_tesseract(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_output(\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextsep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    355\u001b[0m         return_bytes,\n\u001b[0;32m    356\u001b[0m     )\n",
      "File \u001b[1;32m~\\Downloads\\Anaconda3\\Lib\\site-packages\\pytesseract\\pytesseract.py:282\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractNotFoundError()\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timeout_manager(proc, timeout) \u001b[38;5;28;01mas\u001b[39;00m error_string:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode:\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractError(proc\u001b[38;5;241m.\u001b[39mreturncode, get_errors(error_string))\n",
      "File \u001b[1;32m~\\Downloads\\Anaconda3\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Anaconda3\\Lib\\site-packages\\pytesseract\\pytesseract.py:144\u001b[0m, in \u001b[0;36mtimeout_manager\u001b[1;34m(proc, seconds)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m seconds:\n\u001b[1;32m--> 144\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mcommunicate()[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Downloads\\Anaconda3\\Lib\\subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_communicate(\u001b[38;5;28minput\u001b[39m, endtime, timeout)\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Downloads\\Anaconda3\\Lib\\subprocess.py:1628\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001b[39;00m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;66;03m# threads remain reading and the fds left open in case the user\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;66;03m# calls communicate again.\u001b[39;00m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout_thread\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remaining_time(endtime))\n\u001b[0;32m   1629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m   1630\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, orig_timeout)\n",
      "File \u001b[1;32m~\\Downloads\\Anaconda3\\Lib\\threading.py:1149\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\Downloads\\Anaconda3\\Lib\\threading.py:1169\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[0;32m   1170\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.65):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "        # NEW: Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()  # Changed variable name\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords\n",
    "            \"bị\", \"bởi\", \"cả\", \"các\", \"cái\", \"cần\", \"càng\", \"chỉ\", \"chiếc\", \"cho\", \"chứ\", \"chưa\", \"chuyện\", \n",
    "            \"có\", \"có_thể\", \"cứ\", \"của\", \"cùng\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đến_nỗi\", \"đều\", \"điều\", \n",
    "            \"do\", \"đó\", \"được\", \"dưới\", \"gì\", \"khi\", \"không\", \"là\", \"lại\", \"lên\", \"lúc\", \"mà\", \"mỗi\", \n",
    "            \"một_cách\", \"này\", \"nên\", \"nếu\", \"ngay\", \"nhiều\", \"như\", \"nhưng\", \"những\", \"nơi\", \"nữa\", \"phải\", \n",
    "            \"qua\", \"ra\", \"rằng\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"so\", \"sự\", \"tại\", \"theo\", \"thì\", \"trên\", \n",
    "            \"trước\", \"từ\", \"từng\", \"và\", \"vẫn\", \"vào\", \"vậy\", \"vì\", \"việc\", \"với\", \"vừa\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "    \n",
    "    def segment_words(self, text):\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        # Use segmentRawSentences to process the text\n",
    "        segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "        return segmented_text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Vietnamese characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        # Clean text (unchanged)\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords (unchanged)\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Use vws for word segmentation instead of simple split\n",
    "        segmented_text = self.segment_words(text)\n",
    "        \n",
    "        # Return segment words\n",
    "        return segmented_text\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract and preprocess text from PDF file, including text from images\"\"\"\n",
    "        full_text = []\n",
    "        \n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                # Extract regular PDF text\n",
    "                text = \"\"\n",
    "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                for block in blocks:\n",
    "                    if \"lines\" in block:\n",
    "                        for line in block[\"lines\"]:\n",
    "                            for span in line[\"spans\"]:\n",
    "                                text += span[\"text\"] + \" \"\n",
    "                full_text.append(text)\n",
    "                \n",
    "                # Extract text from images in the page\n",
    "                image_list = page.get_images()\n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    try:\n",
    "                        xref = img[0]\n",
    "                        base_image = doc.extract_image(xref)\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        \n",
    "                        # Convert image bytes to OpenCV format\n",
    "                        nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "                        img_np = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "                        \n",
    "                        # === Preprocessing pipeline ===\n",
    "    \n",
    "                        # 1. Grayscale conversion\n",
    "                        gray = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "                        # 2. Brightness normalization (CLAHE)\n",
    "                        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "                        gray = clahe.apply(gray)\n",
    "    \n",
    "                        # 3. Resize to improve Vietnamese diacritic detection\n",
    "                        height = gray.shape[0]\n",
    "                        if height < 1000:\n",
    "                            scale = 300 / 72  # Simulate ~300 DPI if lower res\n",
    "                            gray = cv2.resize(gray, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "                        # 4. Thresholding with Otsu\n",
    "                        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "                        # 5. Deskewing (if needed)\n",
    "                        coords = np.column_stack(np.where(thresh > 0))\n",
    "                        angle = cv2.minAreaRect(coords)[-1]\n",
    "                        if angle < -45:\n",
    "                            angle = 90 + angle\n",
    "                        elif angle > 45:\n",
    "                            angle = angle - 90\n",
    "                        (h, w) = thresh.shape\n",
    "                        M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "                        deskewed = cv2.warpAffine(thresh, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    \n",
    "                        # 6. Light morphological closing to connect broken strokes\n",
    "                        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "                        processed = cv2.morphologyEx(deskewed, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "                        # === Tesseract OCR with layout preservation ===\n",
    "                        d = pytesseract.image_to_data(\n",
    "                            processed,\n",
    "                            lang='vie',\n",
    "                            config='--oem 3 --psm 6',\n",
    "                            output_type=pytesseract.Output.DICT\n",
    "                        )\n",
    "    \n",
    "                        # Group words into structured text (block → line → word)\n",
    "                        blocks = defaultdict(lambda: defaultdict(list))\n",
    "                        for i in range(len(d['text'])):\n",
    "                            if int(d['conf'][i]) > 0 and d['text'][i].strip():\n",
    "                                block_id = d['block_num'][i]\n",
    "                                line_id = d['line_num'][i]\n",
    "                                word_info = {\n",
    "                                    'text': d['text'][i],\n",
    "                                    'left': d['left'][i],\n",
    "                                    'top': d['top'][i]\n",
    "                                }\n",
    "                                blocks[block_id][line_id].append(word_info)\n",
    "    \n",
    "                        # Sort blocks and lines spatially\n",
    "                        sorted_blocks = sorted(blocks.items(), key=lambda b: min([w['top'] for line in b[1].values() for w in line]))\n",
    "                        structured_text = \"\"\n",
    "    \n",
    "                        for block_id, lines in sorted_blocks:\n",
    "                            sorted_lines = sorted(lines.items(), key=lambda l: min(w['top'] for w in l[1]))\n",
    "                            for line_id, words in sorted_lines:\n",
    "                                sorted_words = sorted(words, key=lambda w: w['left'])\n",
    "                                line_text = \" \".join([w['text'] for w in sorted_words])\n",
    "                                structured_text += line_text.strip() + \"\\n\"\n",
    "    \n",
    "                        # Append image-derived text\n",
    "                        full_text.append(structured_text)\n",
    "    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing image {img_index} on page {page.number}: {e}\")\n",
    "                        continue\n",
    "    \n",
    "        return \"\\n\".join(full_text)\n",
    "\n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=1):\n",
    "        \"\"\"Get embeddings in batches\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            \n",
    "            encoded = self.pho_tokenizer(batch, \n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            \n",
    "                # Use mean pooling instead of just [CLS] token\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            batch_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            \n",
    "\n",
    "            # L2 normalization for better cosine similarity\n",
    "            batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "            batch_embeddings = batch_embeddings.numpy()\n",
    "            \n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def read_criteria(self, excel_path):\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()           # A list of criteria is collected\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]      # Process 1 item per time in a list of criteria\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "\n",
    "    def chunk_into_sentences(self, text):\n",
    "        \"\"\"Chunk the text into sentences based on delimiters (. ? ! ; ...).\"\"\"\n",
    "        sentence_endings = re.compile(r'([.!?;]{1,3})')  # Regex to match sentence-ending delimiters\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        sentences = [sentences[i].strip() + (sentences[i + 1] if i + 1 < len(sentences) else '')\n",
    "                     for i in range(0, len(sentences), 2)]  # Only take sentences (even index)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "    def analyze_single_pdf(self, pdf_path, criteria_phrases):\n",
    "        \"\"\"Analyze a single PDF file against criteria phrases\"\"\"\n",
    "        # Extract raw pdf text\n",
    "        pdf_text = self.extract_text_from_pdf(pdf_path)      # full pdf file, raw and original\n",
    "        if not pdf_text:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Initialize results dictionary\n",
    "        results = {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # First, perform exact matching\n",
    "        pdf_text_lower = pdf_text.lower()\n",
    "        for phrase in criteria_phrases:\n",
    "            clean_phrase = self.clean_text(phrase).lower()\n",
    "            if clean_phrase in pdf_text_lower:\n",
    "                results[phrase] = 1\n",
    "        \n",
    "        # Get list of unmatched phrases for semantic analysis\n",
    "        unmatched_phrases = [phrase for phrase in criteria_phrases if results[phrase] == 0]\n",
    "        \n",
    "        # If all phrases were matched exactly, return results\n",
    "        if not unmatched_phrases:\n",
    "            return results\n",
    "            \n",
    "        # Otherwise, proceed with semantic similarity analysis for unmatched phrases\n",
    "        sentences = self.chunk_into_sentences(pdf_text)\n",
    "        if not sentences:\n",
    "            return results\n",
    "    \n",
    "        # Preprocess_text so it is cleaned\n",
    "        processed_text = [self.preprocess_text(senten) for senten in sentences]\n",
    "        \n",
    "        # Get embeddings only for unmatched phrases\n",
    "        try:\n",
    "            unmatched_embeddings = [self.get_embeddings(phrase) for phrase in unmatched_phrases]\n",
    "            \n",
    "            # Check semantic similarity only for unmatched phrases\n",
    "            for sentence in processed_text:\n",
    "                sentence_embedding = self.get_embeddings(sentence)\n",
    "                if len(sentence_embedding.shape) == 1:\n",
    "                    sentence_embedding = sentence_embedding.reshape(1, -1)\n",
    "                for i, criterion_embedding in enumerate(unmatched_embeddings):\n",
    "                    similarity = cosine_similarity(sentence_embedding, criterion_embedding)[0][0]\n",
    "                    if similarity >= self.similarity_threshold:\n",
    "                        results[unmatched_phrases[i]] = 1\n",
    "                \n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic analysis: {e}\")\n",
    "            return results\n",
    "\n",
    "    \n",
    "    def analyze_multiple_pdfs(self, pdf_folder, excel_path, output_path):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        # Read criteria\n",
    "        criteria_phrases = self.read_criteria(excel_path)\n",
    "        if not criteria_phrases:\n",
    "            print(\"No criteria found. Exiting...\")\n",
    "            return\n",
    "        \n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the specified folder.\")\n",
    "            return\n",
    "        \n",
    "        # Process each PDF\n",
    "        all_results = []\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            \n",
    "            # Analyze PDF\n",
    "            results = self.analyze_single_pdf(pdf_path, criteria_phrases)\n",
    "            results['PDF File'] = pdf_file\n",
    "            all_results.append(results)\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            results_df.to_excel(output_path, index=False)\n",
    "            print(f\"Results saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "            \n",
    "#Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.65)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/esg_words.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_analysis_results_exactmatch_imageread.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36642639-d9dc-4f20-b2f9-28f4ed2995ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2011 - HU1.pdf...\n",
      "Processing 2011 - HUT.pdf...\n",
      "Intermediate results saved to esg_pdf_reading.xlsx after processing 2 PDFs.\n",
      "Processing 2011 - HVX.pdf...\n",
      "Processing 2011 - IDI.pdf...\n",
      "Intermediate results saved to esg_pdf_reading.xlsx after processing 4 PDFs.\n",
      "Processing 2011 - IMP.pdf...\n",
      "Processing 2012 - HAS.pdf...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.65):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "        # NEW: Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()  # Changed variable name\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords\n",
    "            \"bị\", \"bởi\", \"cả\", \"các\", \"cái\", \"cần\", \"càng\", \"chỉ\", \"chiếc\", \"cho\", \"chứ\", \"chưa\", \"chuyện\", \n",
    "            \"có\", \"có_thể\", \"cứ\", \"của\", \"cùng\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đến_nỗi\", \"đều\", \"điều\", \n",
    "            \"do\", \"đó\", \"được\", \"dưới\", \"gì\", \"khi\", \"không\", \"là\", \"lại\", \"lên\", \"lúc\", \"mà\", \"mỗi\", \n",
    "            \"một_cách\", \"này\", \"nên\", \"nếu\", \"ngay\", \"nhiều\", \"như\", \"nhưng\", \"những\", \"nơi\", \"nữa\", \"phải\", \n",
    "            \"qua\", \"ra\", \"rằng\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"so\", \"sự\", \"tại\", \"theo\", \"thì\", \"trên\", \n",
    "            \"trước\", \"từ\", \"từng\", \"và\", \"vẫn\", \"vào\", \"vậy\", \"vì\", \"việc\", \"với\", \"vừa\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "    \n",
    "    def segment_words(self, text):\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        # Use segmentRawSentences to process the text\n",
    "        segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "        return segmented_text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Vietnamese characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        # Clean text (unchanged)\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords (unchanged)\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Use vws for word segmentation instead of simple split\n",
    "        segmented_text = self.segment_words(text)\n",
    "        \n",
    "        # Return segment words\n",
    "        return segmented_text\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract and preprocess text from PDF file, including text from images\"\"\"\n",
    "        full_text = []\n",
    "        \n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                # Extract regular PDF text\n",
    "                text = \"\"\n",
    "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                for block in blocks:\n",
    "                    if \"lines\" in block:\n",
    "                        for line in block[\"lines\"]:\n",
    "                            for span in line[\"spans\"]:\n",
    "                                text += span[\"text\"] + \" \"\n",
    "                full_text.append(text)\n",
    "                \n",
    "                # Extract text from images in the page\n",
    "                image_list = page.get_images()\n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    try:\n",
    "                        xref = img[0]\n",
    "                        base_image = doc.extract_image(xref)\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        \n",
    "                        # Convert image bytes to OpenCV format\n",
    "                        nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "                        img_np = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "                        \n",
    "                        # === Preprocessing pipeline ===\n",
    "    \n",
    "                        # 1. Grayscale conversion\n",
    "                        gray = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "                        # 2. Brightness normalization (CLAHE)\n",
    "                        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "                        gray = clahe.apply(gray)\n",
    "    \n",
    "                        # 3. Resize to improve Vietnamese diacritic detection\n",
    "                        height = gray.shape[0]\n",
    "                        if height < 1000:\n",
    "                            scale = 300 / 72  # Simulate ~300 DPI if lower res\n",
    "                            gray = cv2.resize(gray, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "                        # 4. Thresholding with Otsu\n",
    "                        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "                        # 5. Deskewing (if needed)\n",
    "                        coords = np.column_stack(np.where(thresh > 0))\n",
    "                        angle = cv2.minAreaRect(coords)[-1]\n",
    "                        if angle < -45:\n",
    "                            angle = 90 + angle\n",
    "                        elif angle > 45:\n",
    "                            angle = angle - 90\n",
    "                        (h, w) = thresh.shape\n",
    "                        M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "                        deskewed = cv2.warpAffine(thresh, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    \n",
    "                        # 6. Light morphological closing to connect broken strokes\n",
    "                        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "                        processed = cv2.morphologyEx(deskewed, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "                        # === Tesseract OCR with layout preservation ===\n",
    "                        d = pytesseract.image_to_data(\n",
    "                            processed,\n",
    "                            lang='vie',\n",
    "                            config='--oem 3 --psm 6',\n",
    "                            output_type=pytesseract.Output.DICT\n",
    "                        )\n",
    "    \n",
    "                        # Group words into structured text (block → line → word)\n",
    "                        blocks = defaultdict(lambda: defaultdict(list))\n",
    "                        for i in range(len(d['text'])):\n",
    "                            if int(d['conf'][i]) > 0 and d['text'][i].strip():\n",
    "                                block_id = d['block_num'][i]\n",
    "                                line_id = d['line_num'][i]\n",
    "                                word_info = {\n",
    "                                    'text': d['text'][i],\n",
    "                                    'left': d['left'][i],\n",
    "                                    'top': d['top'][i]\n",
    "                                }\n",
    "                                blocks[block_id][line_id].append(word_info)\n",
    "    \n",
    "                        # Sort blocks and lines spatially\n",
    "                        sorted_blocks = sorted(blocks.items(), key=lambda b: min([w['top'] for line in b[1].values() for w in line]))\n",
    "                        structured_text = \"\"\n",
    "    \n",
    "                        for block_id, lines in sorted_blocks:\n",
    "                            sorted_lines = sorted(lines.items(), key=lambda l: min(w['top'] for w in l[1]))\n",
    "                            for line_id, words in sorted_lines:\n",
    "                                sorted_words = sorted(words, key=lambda w: w['left'])\n",
    "                                line_text = \" \".join([w['text'] for w in sorted_words])\n",
    "                                structured_text += line_text.strip() + \"\\n\"\n",
    "    \n",
    "                        # Append image-derived text\n",
    "                        full_text.append(structured_text)\n",
    "    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing image {img_index} on page {page.number}: {e}\")\n",
    "                        continue\n",
    "    \n",
    "        return \"\\n\".join(full_text)\n",
    "\n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=1):\n",
    "        \"\"\"Get embeddings in batches\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            \n",
    "            encoded = self.pho_tokenizer(batch, \n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            \n",
    "                # Use mean pooling instead of just [CLS] token\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            batch_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            \n",
    "\n",
    "            # L2 normalization for better cosine similarity\n",
    "            batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "            batch_embeddings = batch_embeddings.numpy()\n",
    "            \n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def read_criteria(self, excel_path):\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()           # A list of criteria is collected\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]      # Process 1 item per time in a list of criteria\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "\n",
    "    def chunk_into_sentences(self, text):\n",
    "        \"\"\"Chunk the text into sentences based on delimiters (. ? ! ; ...).\"\"\"\n",
    "        sentence_endings = re.compile(r'([.!?;]{1,3})')  # Regex to match sentence-ending delimiters\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        sentences = [sentences[i].strip() + (sentences[i + 1] if i + 1 < len(sentences) else '')\n",
    "                     for i in range(0, len(sentences), 2)]  # Only take sentences (even index)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "    def analyze_single_pdf(self, pdf_path, criteria_phrases):\n",
    "        \"\"\"Analyze a single PDF file against criteria phrases\"\"\"\n",
    "        # Extract raw pdf text\n",
    "        pdf_text = self.extract_text_from_pdf(pdf_path)      # full pdf file, raw and original\n",
    "        if not pdf_text:\n",
    "            return {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # Initialize results dictionary\n",
    "        results = {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # First, perform exact matching\n",
    "        pdf_text_lower = pdf_text.lower()\n",
    "        for phrase in criteria_phrases:\n",
    "            clean_phrase = self.clean_text(phrase).lower()\n",
    "            if clean_phrase in pdf_text_lower:\n",
    "                results[phrase] = 1\n",
    "        \n",
    "        # Get list of unmatched phrases for semantic analysis\n",
    "        unmatched_phrases = [phrase for phrase in criteria_phrases if results[phrase] == 0]\n",
    "        \n",
    "        # If all phrases were matched exactly, return results\n",
    "        if not unmatched_phrases:\n",
    "            return results\n",
    "            \n",
    "        # Otherwise, proceed with semantic similarity analysis for unmatched phrases\n",
    "        sentences = self.chunk_into_sentences(pdf_text)\n",
    "        if not sentences:\n",
    "            return results\n",
    "    \n",
    "        # Preprocess_text so it is cleaned\n",
    "        processed_text = [self.preprocess_text(senten) for senten in sentences]\n",
    "        \n",
    "        # Get embeddings only for unmatched phrases\n",
    "        try:\n",
    "            unmatched_embeddings = [self.get_embeddings(phrase) for phrase in unmatched_phrases]\n",
    "            \n",
    "            # Check semantic similarity only for unmatched phrases\n",
    "            for sentence in processed_text:\n",
    "                sentence_embedding = self.get_embeddings(sentence)\n",
    "                if len(sentence_embedding.shape) == 1:\n",
    "                    sentence_embedding = sentence_embedding.reshape(1, -1)\n",
    "                for i, criterion_embedding in enumerate(unmatched_embeddings):\n",
    "                    similarity = cosine_similarity(sentence_embedding, criterion_embedding)[0][0]\n",
    "                    if similarity >= self.similarity_threshold:\n",
    "                        results[unmatched_phrases[i]] = 1\n",
    "                \n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic analysis: {e}\")\n",
    "            return results\n",
    "\n",
    "    \n",
    "    def analyze_multiple_pdfs(self, pdf_folder, excel_path, output_path):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        # Read criteria\n",
    "        criteria_phrases = self.read_criteria(excel_path)\n",
    "        if not criteria_phrases:\n",
    "            print(\"No criteria found. Exiting...\")\n",
    "            return\n",
    "\n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the specified folder.\")\n",
    "            return\n",
    "            \n",
    "        all_results = []   \n",
    "        for idx, pdf_file in enumerate(pdf_files, 1):  # Start index at 1\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        \n",
    "            # Analyze PDF\n",
    "            results = self.analyze_single_pdf(pdf_path, criteria_phrases)\n",
    "            results['PDF File'] = pdf_file\n",
    "            all_results.append(results)\n",
    "        \n",
    "            # Save after every 3 files OR at the end of the list\n",
    "            if idx % 2 == 0 or idx == len(pdf_files): \n",
    "                try:\n",
    "                    results_df = pd.DataFrame(all_results)\n",
    "                    results_df.to_excel(output_path, index=False)\n",
    "                    print(f\"Intermediate results saved to {output_path} after processing {idx} PDFs.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving results after {idx} PDFs: {e}\")\n",
    "            \n",
    "#Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.65)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/esg_words.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_pdf_reading.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2ab2f-6954-49da-a8ff-c8051f253eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF, transformers==4.50.3, torch==2.6.0, pytesseract, pdf2image, opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ea044-3728-427e-a18f-607d01effb1f",
   "metadata": {},
   "source": [
    "# Final V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6decdf-7aca-4924-8685-86a9be564c19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vws import RDRSegmenter, Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class ESGAnalyzer:\n",
    "    def __init__(self, similarity_threshold=0.65):\n",
    "        # Initialize PhoBERT\n",
    "        self.pho_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\") \n",
    "        self.model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "        # NEW: Initialize Vietnamese word segmenter\n",
    "        self.rdrsegment = RDRSegmenter.RDRSegmenter()  # Changed variable name\n",
    "        self.token = Tokenizer.Tokenizer()\n",
    "        \n",
    "        # Vietnamese stopwords - expanded for ESG context\n",
    "        self.stopwords = set([\n",
    "            # Common Vietnamese stopwords           \n",
    "        \"bị\", \"bởi\", \"cả\", \"các\", \"có\", \"có_thể\", \"có_lẽ\", \"của\",\n",
    "        \"cùng\",\"cùng_với\", \"cũng\", \"đã\", \"đang\", \"đây\", \"để\", \"đều\", \"do\", \"đó\",\n",
    "        \"khi\", \"là\", \"lại\", \"mà\", \"nên\", \"nếu\", \"những\",\n",
    "        \"phải\", \"rất\", \"rồi\", \"sau\", \"sẽ\", \"thì\", \"từ\", \"và\"\n",
    "        ])\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "\n",
    "    \n",
    "    def segment_words(self, text):\n",
    "        \"\"\"Segment Vietnamese text using vws\"\"\"\n",
    "        # Use segmentRawSentences to process the text\n",
    "        segmented_text = self.rdrsegment.segmentRawSentences(self.token, text)\n",
    "        return segmented_text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize Vietnamese text for ESG content\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep Vietnamese characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s\\dàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove Vietnamese stopwords\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stopwords]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "        # Clean text (unchanged)\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords (unchanged)\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Use vws for word segmentation instead of simple split\n",
    "        segmented_text = self.segment_words(text)\n",
    "        \n",
    "        # Return segment words\n",
    "        return segmented_text\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract and preprocess text from PDF, handling both vector and image-only pages\"\"\"\n",
    "\n",
    "        full_text = []\n",
    "        try:\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                for page in doc:\n",
    "                    # === 1. Extract Vector Text ===\n",
    "                    text = \"\"\n",
    "                    blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                    for block in blocks:\n",
    "                        if \"lines\" in block:\n",
    "                            for line in block[\"lines\"]:\n",
    "                                for span in line[\"spans\"]:\n",
    "                                    text += span[\"text\"] + \" \"\n",
    "                    full_text.append(text.strip())\n",
    "        \n",
    "                    # === 2. OCR Page as Image (Visual Handling) ===\n",
    "                    try:\n",
    "                        zoom = 2.0  # Higher DPI (144 DPI)\n",
    "                        mat = fitz.Matrix(zoom, zoom)\n",
    "                        pix = page.get_pixmap(matrix=mat)\n",
    "        \n",
    "                        img_np = np.frombuffer(pix.samples, dtype=np.uint8).reshape((pix.height, pix.width, pix.n))\n",
    "                        if pix.n == 4:\n",
    "                            img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2BGR)\n",
    "                        elif pix.n == 1:\n",
    "                            img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "                        # Quick precheck (low-cost confidence screening)\n",
    "                        quick_ocr = pytesseract.image_to_data(\n",
    "                            img_np, lang='vie', config='--oem 3 --psm 6', output_type=pytesseract.Output.DICT\n",
    "                        )\n",
    "                        quick_valid_chars = sum(len(word.strip()) for word, conf in zip(quick_ocr['text'], quick_ocr['conf']) if word.strip() and int(conf) > 60)\n",
    "        \n",
    "                        if quick_valid_chars < 6:\n",
    "                            continue\n",
    "        \n",
    "                        # === Preprocessing for final OCR ===\n",
    "                        gray = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
    "                        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "                        gray = clahe.apply(gray)\n",
    "        \n",
    "                        # Upscale if too small\n",
    "                        if gray.shape[0] < 1000:\n",
    "                            scale = 300 / 72\n",
    "                            gray = cv2.resize(gray, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "                        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "        \n",
    "                        coords = np.column_stack(np.where(thresh > 0))\n",
    "                        angle = cv2.minAreaRect(coords)[-1]\n",
    "                        if angle < -45:\n",
    "                            angle += 90\n",
    "                        elif angle > 45:\n",
    "                            angle -= 90\n",
    "                        (h, w) = thresh.shape\n",
    "                        M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "                        deskewed = cv2.warpAffine(thresh, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "        \n",
    "                        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "                        processed = cv2.morphologyEx(deskewed, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "                        d = pytesseract.image_to_data(\n",
    "                            processed, lang='vie', config='--oem 3 --psm 6', output_type=pytesseract.Output.DICT\n",
    "                        )\n",
    "        \n",
    "                        # === Filter out meaningless pages ===\n",
    "                        valid_word_count = 0\n",
    "                        valid_char_count = 0\n",
    "                        for i in range(len(d['text'])):\n",
    "                            word = d['text'][i].strip()\n",
    "                            conf = int(d['conf'][i])\n",
    "                            if conf >= 70 and len(word) >= 3:\n",
    "                                valid_word_count += 1\n",
    "                                valid_char_count += len(word)\n",
    "        \n",
    "                        if valid_word_count < 3 or valid_char_count < 17:\n",
    "                            continue\n",
    "        \n",
    "                        # === Structure OCR Text ===\n",
    "                        blocks = defaultdict(lambda: defaultdict(list))\n",
    "                        for i in range(len(d['text'])):\n",
    "                            if int(d['conf'][i]) > 0 and d['text'][i].strip():\n",
    "                                block_id = d['block_num'][i]\n",
    "                                line_id = d['line_num'][i]\n",
    "                                word_info = {\n",
    "                                    'text': d['text'][i],\n",
    "                                    'left': d['left'][i],\n",
    "                                    'top': d['top'][i]\n",
    "                                }\n",
    "                                blocks[block_id][line_id].append(word_info)\n",
    "        \n",
    "                        sorted_blocks = sorted(blocks.items(), key=lambda b: min([w['top'] for line in b[1].values() for w in line]))\n",
    "                        structured_text = \"\"\n",
    "                        for block_id, lines in sorted_blocks:\n",
    "                            sorted_lines = sorted(lines.items(), key=lambda l: min(w['top'] for w in l[1]))\n",
    "                            for line_id, words in sorted_lines:\n",
    "                                sorted_words = sorted(words, key=lambda w: w['left'])\n",
    "                                line_text = \" \".join([w['text'] for w in sorted_words])\n",
    "                                structured_text += line_text.strip() + \"\\n\"\n",
    "        \n",
    "                        full_text.append(structured_text)\n",
    "        \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Error rendering or OCRing page {page.number}: {e}\")\n",
    "                        continue\n",
    "        \n",
    "            return \"\\n\".join(full_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped empty file: {pdf_path}\")\n",
    "            \n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=1):\n",
    "        \"\"\"Get embeddings in batches\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            \n",
    "            encoded = self.pho_tokenizer(batch, \n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            \n",
    "                # Use mean pooling instead of just [CLS] token\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            batch_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            \n",
    "\n",
    "            # L2 normalization for better cosine similarity\n",
    "            batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "            batch_embeddings = batch_embeddings.numpy()\n",
    "            \n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def read_criteria(self, excel_path):\n",
    "        \"\"\"Read and preprocess ESG criteria from Excel file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_path)\n",
    "            # Ensure we're reading just one column\n",
    "            if 'criteria' in df.columns:\n",
    "                criteria = df['criteria'].tolist()           # A list of criteria is collected\n",
    "            else:\n",
    "                # If 'criteria' column doesn't exist, take the first column\n",
    "                criteria = df.iloc[:, 0].tolist()\n",
    "            \n",
    "            processed_criteria = [self.preprocess_text(str(c)) for c in criteria]      # Process 1 item per time in a list of criteria\n",
    "            return processed_criteria\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading criteria file: {e}\")\n",
    "            return []\n",
    "\n",
    "    def chunk_into_sentences(self, text):\n",
    "        \"\"\"Chunk the text into sentences based on delimiters (. ? ! ; ...).\"\"\"\n",
    "        sentence_endings = re.compile(r'([.!?;]{1,3})')  # Regex to match sentence-ending delimiters\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        sentences = [sentences[i].strip() + (sentences[i + 1] if i + 1 < len(sentences) else '')\n",
    "                     for i in range(0, len(sentences), 2)]  # Only take sentences (even index)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "    def analyze_single_pdf(self, pdf_path, criteria_phrases):\n",
    "        \"\"\"Analyze a single PDF file against criteria phrases\"\"\"\n",
    "        # Extract raw pdf text\n",
    "        pdf_text = self.extract_text_from_pdf(pdf_path)      # full pdf file, raw and original\n",
    "        if not pdf_text:\n",
    "            return {phrase: \"x\" for phrase in criteria_phrases}\n",
    "\n",
    "                 \n",
    "        # Initialize results dictionary\n",
    "        results = {phrase: 0 for phrase in criteria_phrases}\n",
    "        \n",
    "        # First, perform exact matching\n",
    "        pdf_text_lower = pdf_text.lower()\n",
    "        for phrase in criteria_phrases:\n",
    "            clean_phrase = self.clean_text(phrase).lower()\n",
    "            if clean_phrase in pdf_text_lower:\n",
    "                results[phrase] = 1\n",
    "        \n",
    "        # Get list of unmatched phrases for semantic analysis\n",
    "        unmatched_phrases = [phrase for phrase in criteria_phrases if results[phrase] == 0]\n",
    "        \n",
    "        # If all phrases were matched exactly, return results\n",
    "        if not unmatched_phrases:\n",
    "            return results\n",
    "            \n",
    "        # Otherwise, proceed with semantic similarity analysis for unmatched phrases\n",
    "        sentences = self.chunk_into_sentences(pdf_text)\n",
    "        if not sentences:\n",
    "            return results\n",
    "    \n",
    "        # Preprocess_text so it is cleaned\n",
    "        processed_text = [self.preprocess_text(senten) for senten in sentences]\n",
    "        \n",
    "        # Get embeddings only for unmatched phrases\n",
    "        try:\n",
    "            unmatched_embeddings = [self.get_embeddings(phrase) for phrase in unmatched_phrases]\n",
    "            \n",
    "            # Check semantic similarity only for unmatched phrases\n",
    "            for sentence in processed_text:\n",
    "                sentence_embedding = self.get_embeddings(sentence)\n",
    "                if len(sentence_embedding.shape) == 1:\n",
    "                    sentence_embedding = sentence_embedding.reshape(1, -1)\n",
    "                for i, criterion_embedding in enumerate(unmatched_embeddings):\n",
    "                    similarity = cosine_similarity(sentence_embedding, criterion_embedding)[0][0]\n",
    "                    if similarity >= self.similarity_threshold:\n",
    "                        results[unmatched_phrases[i]] = 1\n",
    "                \n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic analysis: {e}\")\n",
    "            return results\n",
    "\n",
    "    \n",
    "    def analyze_multiple_pdfs(self, pdf_folder, excel_path, output_path):\n",
    "        \"\"\"Analyze multiple PDF files and save results\"\"\"\n",
    "        # Read criteria\n",
    "        criteria_phrases = self.read_criteria(excel_path)\n",
    "        if not criteria_phrases:\n",
    "            print(\"No criteria found. Exiting...\")\n",
    "            return\n",
    "\n",
    "        # Get list of PDF files\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the specified folder.\")\n",
    "            return\n",
    "            \n",
    "        all_results = []   \n",
    "        for idx, pdf_file in enumerate(pdf_files, 1):  # Start index at 1\n",
    "            print(f\"Processing {pdf_file}...\")\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        \n",
    "            # Analyze PDF\n",
    "            results = self.analyze_single_pdf(pdf_path, criteria_phrases)\n",
    "            results['PDF File'] = pdf_file\n",
    "            all_results.append(results)\n",
    "        \n",
    "            # Save after every 2 files OR at the end of the list\n",
    "            if idx % 2 == 0 or idx == len(pdf_files): \n",
    "                try:\n",
    "                    results_df = pd.DataFrame(all_results)\n",
    "                    results_df.to_excel(output_path, index=False)\n",
    "                    print(f\"Intermediate results saved to {output_path} after processing {idx} PDFs.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving results after {idx} PDFs: {e}\")\n",
    "            \n",
    "#Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ESGAnalyzer(similarity_threshold=0.65)\n",
    "    \n",
    "    # Set paths\n",
    "    pdf_folder = \"Desktop/ESG pdf\"  # Replace with your PDF folder path\n",
    "    excel_path = \"Desktop/esg_words.xlsx\"  # Replace with your Excel file path\n",
    "    output_path = \"esg_pdf_reading.xlsx\"\n",
    "    \n",
    "    # Run analysis\n",
    "analyzer.analyze_multiple_pdfs(pdf_folder, excel_path, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
